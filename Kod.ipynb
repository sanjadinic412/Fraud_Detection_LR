{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "card = pd.read_csv(r'C:\\Users\\pc\\OneDrive\\Desktop\\folderi\\FON\\mfs\\Kod\\creditcard.csv')\n",
    "card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
    "\n",
    "X1 = card[estimators]\n",
    "y = card['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col=X1.columns[:-1]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003914\n",
      "         Iterations 13\n"
     ]
    }
   ],
   "source": [
    "X = sm.add_constant(X1)\n",
    "reg_logit = sm.Logit(y,X)\n",
    "results_logit = reg_logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>284807</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>284776</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    30</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 22 May 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.6922</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:03:12</td>     <th>  Log-Likelihood:    </th> <td> -1114.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -3621.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -8.3917</td> <td>    0.249</td> <td>  -33.652</td> <td> 0.000</td> <td>   -8.880</td> <td>   -7.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th>   <td>-3.742e-06</td> <td> 2.26e-06</td> <td>   -1.659</td> <td> 0.097</td> <td>-8.16e-06</td> <td> 6.79e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>     <td>    0.0960</td> <td>    0.042</td> <td>    2.264</td> <td> 0.024</td> <td>    0.013</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>     <td>    0.0094</td> <td>    0.058</td> <td>    0.161</td> <td> 0.872</td> <td>   -0.104</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>     <td>   -0.0079</td> <td>    0.053</td> <td>   -0.149</td> <td> 0.881</td> <td>   -0.112</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>     <td>    0.6986</td> <td>    0.074</td> <td>    9.454</td> <td> 0.000</td> <td>    0.554</td> <td>    0.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>     <td>    0.1295</td> <td>    0.067</td> <td>    1.944</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>     <td>   -0.1198</td> <td>    0.074</td> <td>   -1.626</td> <td> 0.104</td> <td>   -0.264</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>     <td>   -0.0969</td> <td>    0.067</td> <td>   -1.453</td> <td> 0.146</td> <td>   -0.228</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>     <td>   -0.1739</td> <td>    0.030</td> <td>   -5.711</td> <td> 0.000</td> <td>   -0.234</td> <td>   -0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>     <td>   -0.2843</td> <td>    0.111</td> <td>   -2.561</td> <td> 0.010</td> <td>   -0.502</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>    <td>   -0.8176</td> <td>    0.097</td> <td>   -8.432</td> <td> 0.000</td> <td>   -1.008</td> <td>   -0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>    <td>   -0.0621</td> <td>    0.081</td> <td>   -0.762</td> <td> 0.446</td> <td>   -0.222</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>    <td>    0.0909</td> <td>    0.087</td> <td>    1.045</td> <td> 0.296</td> <td>   -0.080</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>    <td>   -0.3312</td> <td>    0.082</td> <td>   -4.058</td> <td> 0.000</td> <td>   -0.491</td> <td>   -0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>    <td>   -0.5571</td> <td>    0.062</td> <td>   -8.949</td> <td> 0.000</td> <td>   -0.679</td> <td>   -0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>    <td>   -0.1141</td> <td>    0.086</td> <td>   -1.330</td> <td> 0.183</td> <td>   -0.282</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>    <td>   -0.1908</td> <td>    0.125</td> <td>   -1.526</td> <td> 0.127</td> <td>   -0.436</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>    <td>   -0.0216</td> <td>    0.070</td> <td>   -0.309</td> <td> 0.757</td> <td>   -0.159</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>    <td>   -0.0131</td> <td>    0.129</td> <td>   -0.102</td> <td> 0.919</td> <td>   -0.266</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>    <td>    0.0963</td> <td>    0.097</td> <td>    0.993</td> <td> 0.321</td> <td>   -0.094</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>    <td>   -0.4582</td> <td>    0.082</td> <td>   -5.607</td> <td> 0.000</td> <td>   -0.618</td> <td>   -0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>    <td>    0.3898</td> <td>    0.060</td> <td>    6.494</td> <td> 0.000</td> <td>    0.272</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>    <td>    0.6297</td> <td>    0.134</td> <td>    4.707</td> <td> 0.000</td> <td>    0.367</td> <td>    0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>    <td>   -0.0951</td> <td>    0.058</td> <td>   -1.629</td> <td> 0.103</td> <td>   -0.209</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>    <td>    0.1289</td> <td>    0.147</td> <td>    0.874</td> <td> 0.382</td> <td>   -0.160</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>    <td>   -0.0761</td> <td>    0.131</td> <td>   -0.582</td> <td> 0.560</td> <td>   -0.332</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>    <td>    0.0195</td> <td>    0.190</td> <td>    0.103</td> <td> 0.918</td> <td>   -0.352</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>    <td>   -0.8188</td> <td>    0.122</td> <td>   -6.686</td> <td> 0.000</td> <td>   -1.059</td> <td>   -0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>    <td>   -0.2937</td> <td>    0.088</td> <td>   -3.332</td> <td> 0.001</td> <td>   -0.467</td> <td>   -0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th> <td>    0.0009</td> <td>    0.000</td> <td>    2.449</td> <td> 0.014</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.31 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               284807\n",
       "Model:                          Logit   Df Residuals:                   284776\n",
       "Method:                           MLE   Df Model:                           30\n",
       "Date:                Sat, 22 May 2021   Pseudo R-squ.:                  0.6922\n",
       "Time:                        16:03:12   Log-Likelihood:                -1114.8\n",
       "converged:                       True   LL-Null:                       -3621.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -8.3917      0.249    -33.652      0.000      -8.880      -7.903\n",
       "Time       -3.742e-06   2.26e-06     -1.659      0.097   -8.16e-06    6.79e-07\n",
       "V1             0.0960      0.042      2.264      0.024       0.013       0.179\n",
       "V2             0.0094      0.058      0.161      0.872      -0.104       0.123\n",
       "V3            -0.0079      0.053     -0.149      0.881      -0.112       0.096\n",
       "V4             0.6986      0.074      9.454      0.000       0.554       0.843\n",
       "V5             0.1295      0.067      1.944      0.052      -0.001       0.260\n",
       "V6            -0.1198      0.074     -1.626      0.104      -0.264       0.025\n",
       "V7            -0.0969      0.067     -1.453      0.146      -0.228       0.034\n",
       "V8            -0.1739      0.030     -5.711      0.000      -0.234      -0.114\n",
       "V9            -0.2843      0.111     -2.561      0.010      -0.502      -0.067\n",
       "V10           -0.8176      0.097     -8.432      0.000      -1.008      -0.628\n",
       "V11           -0.0621      0.081     -0.762      0.446      -0.222       0.098\n",
       "V12            0.0909      0.087      1.045      0.296      -0.080       0.261\n",
       "V13           -0.3312      0.082     -4.058      0.000      -0.491      -0.171\n",
       "V14           -0.5571      0.062     -8.949      0.000      -0.679      -0.435\n",
       "V15           -0.1141      0.086     -1.330      0.183      -0.282       0.054\n",
       "V16           -0.1908      0.125     -1.526      0.127      -0.436       0.054\n",
       "V17           -0.0216      0.070     -0.309      0.757      -0.159       0.116\n",
       "V18           -0.0131      0.129     -0.102      0.919      -0.266       0.240\n",
       "V19            0.0963      0.097      0.993      0.321      -0.094       0.286\n",
       "V20           -0.4582      0.082     -5.607      0.000      -0.618      -0.298\n",
       "V21            0.3898      0.060      6.494      0.000       0.272       0.507\n",
       "V22            0.6297      0.134      4.707      0.000       0.367       0.892\n",
       "V23           -0.0951      0.058     -1.629      0.103      -0.209       0.019\n",
       "V24            0.1289      0.147      0.874      0.382      -0.160       0.418\n",
       "V25           -0.0761      0.131     -0.582      0.560      -0.332       0.180\n",
       "V26            0.0195      0.190      0.103      0.918      -0.352       0.392\n",
       "V27           -0.8188      0.122     -6.686      0.000      -1.059      -0.579\n",
       "V28           -0.2937      0.088     -3.332      0.001      -0.467      -0.121\n",
       "Amount         0.0009      0.000      2.449      0.014       0.000       0.002\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.31 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_logit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_feature_elem (data_frame,dep_var,col_list):\n",
    "    \n",
    "\n",
    "    while len(col_list)>0 :\n",
    "        model=sm.Logit(dep_var,data_frame[col_list])\n",
    "        result=model.fit(disp=0)\n",
    "        largest_pvalue=round(result.pvalues,3).nlargest(1)\n",
    "        if largest_pvalue[0]<(0.0001):\n",
    "            return result\n",
    "            break\n",
    "        else:\n",
    "            col_list=col_list.drop(largest_pvalue.index)\n",
    "\n",
    "result=back_feature_elem(X,card.Class,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>284807</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>284782</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    24</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 22 May 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.06233</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:05:00</td>     <th>  Log-Likelihood:    </th> <td> -3395.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -3621.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.919e-80</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th> <td>   -0.0001</td> <td> 1.43e-06</td> <td>  -86.049</td> <td> 0.000</td> <td>   -0.000</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>   <td>    0.8905</td> <td>    0.028</td> <td>   31.948</td> <td> 0.000</td> <td>    0.836</td> <td>    0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>   <td>   -0.4531</td> <td>    0.023</td> <td>  -19.454</td> <td> 0.000</td> <td>   -0.499</td> <td>   -0.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>   <td>   -1.6040</td> <td>    0.032</td> <td>  -49.454</td> <td> 0.000</td> <td>   -1.668</td> <td>   -1.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>   <td>    0.1445</td> <td>    0.025</td> <td>    5.668</td> <td> 0.000</td> <td>    0.095</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>   <td>    0.4081</td> <td>    0.024</td> <td>   17.097</td> <td> 0.000</td> <td>    0.361</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>   <td>   -0.3890</td> <td>    0.025</td> <td>  -15.311</td> <td> 0.000</td> <td>   -0.439</td> <td>   -0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>   <td>    0.0992</td> <td>    0.028</td> <td>    3.582</td> <td> 0.000</td> <td>    0.045</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>   <td>   -0.3893</td> <td>    0.023</td> <td>  -17.118</td> <td> 0.000</td> <td>   -0.434</td> <td>   -0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>   <td>   -0.4601</td> <td>    0.043</td> <td>  -10.675</td> <td> 0.000</td> <td>   -0.545</td> <td>   -0.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>  <td>   -0.3799</td> <td>    0.051</td> <td>   -7.436</td> <td> 0.000</td> <td>   -0.480</td> <td>   -0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>  <td>   -0.6133</td> <td>    0.034</td> <td>  -17.969</td> <td> 0.000</td> <td>   -0.680</td> <td>   -0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>  <td>    0.1263</td> <td>    0.034</td> <td>    3.678</td> <td> 0.000</td> <td>    0.059</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>  <td>   -0.4513</td> <td>    0.035</td> <td>  -13.070</td> <td> 0.000</td> <td>   -0.519</td> <td>   -0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>  <td>   -0.6980</td> <td>    0.032</td> <td>  -22.083</td> <td> 0.000</td> <td>   -0.760</td> <td>   -0.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>  <td>   -1.0426</td> <td>    0.041</td> <td>  -25.255</td> <td> 0.000</td> <td>   -1.123</td> <td>   -0.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>  <td>   -0.2386</td> <td>    0.042</td> <td>   -5.747</td> <td> 0.000</td> <td>   -0.320</td> <td>   -0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>  <td>   -0.7041</td> <td>    0.033</td> <td>  -21.507</td> <td> 0.000</td> <td>   -0.768</td> <td>   -0.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>  <td>   -0.6758</td> <td>    0.050</td> <td>  -13.576</td> <td> 0.000</td> <td>   -0.773</td> <td>   -0.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>  <td>    0.5683</td> <td>    0.041</td> <td>   13.871</td> <td> 0.000</td> <td>    0.488</td> <td>    0.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>  <td>    1.3555</td> <td>    0.062</td> <td>   21.869</td> <td> 0.000</td> <td>    1.234</td> <td>    1.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>  <td>    0.2932</td> <td>    0.054</td> <td>    5.408</td> <td> 0.000</td> <td>    0.187</td> <td>    0.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>  <td>   -1.9974</td> <td>    0.072</td> <td>  -27.593</td> <td> 0.000</td> <td>   -2.139</td> <td>   -1.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>  <td>    0.3076</td> <td>    0.072</td> <td>    4.281</td> <td> 0.000</td> <td>    0.167</td> <td>    0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>  <td>   -0.7474</td> <td>    0.088</td> <td>   -8.483</td> <td> 0.000</td> <td>   -0.920</td> <td>   -0.575</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.68 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               284807\n",
       "Model:                          Logit   Df Residuals:                   284782\n",
       "Method:                           MLE   Df Model:                           24\n",
       "Date:                Sat, 22 May 2021   Pseudo R-squ.:                 0.06233\n",
       "Time:                        16:05:00   Log-Likelihood:                -3395.5\n",
       "converged:                       True   LL-Null:                       -3621.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.919e-80\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Time          -0.0001   1.43e-06    -86.049      0.000      -0.000      -0.000\n",
       "V1             0.8905      0.028     31.948      0.000       0.836       0.945\n",
       "V2            -0.4531      0.023    -19.454      0.000      -0.499      -0.407\n",
       "V3            -1.6040      0.032    -49.454      0.000      -1.668      -1.540\n",
       "V4             0.1445      0.025      5.668      0.000       0.095       0.195\n",
       "V5             0.4081      0.024     17.097      0.000       0.361       0.455\n",
       "V6            -0.3890      0.025    -15.311      0.000      -0.439      -0.339\n",
       "V7             0.0992      0.028      3.582      0.000       0.045       0.154\n",
       "V8            -0.3893      0.023    -17.118      0.000      -0.434      -0.345\n",
       "V9            -0.4601      0.043    -10.675      0.000      -0.545      -0.376\n",
       "V10           -0.3799      0.051     -7.436      0.000      -0.480      -0.280\n",
       "V11           -0.6133      0.034    -17.969      0.000      -0.680      -0.546\n",
       "V12            0.1263      0.034      3.678      0.000       0.059       0.194\n",
       "V13           -0.4513      0.035    -13.070      0.000      -0.519      -0.384\n",
       "V14           -0.6980      0.032    -22.083      0.000      -0.760      -0.636\n",
       "V15           -1.0426      0.041    -25.255      0.000      -1.123      -0.962\n",
       "V16           -0.2386      0.042     -5.747      0.000      -0.320      -0.157\n",
       "V17           -0.7041      0.033    -21.507      0.000      -0.768      -0.640\n",
       "V20           -0.6758      0.050    -13.576      0.000      -0.773      -0.578\n",
       "V21            0.5683      0.041     13.871      0.000       0.488       0.649\n",
       "V22            1.3555      0.062     21.869      0.000       1.234       1.477\n",
       "V23            0.2932      0.054      5.408      0.000       0.187       0.399\n",
       "V25           -1.9974      0.072    -27.593      0.000      -2.139      -1.855\n",
       "V26            0.3076      0.072      4.281      0.000       0.167       0.448\n",
       "V27           -0.7474      0.088     -8.483      0.000      -0.920      -0.575\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.68 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CI 95%(2.5%)  CI 95%(97.5%)  Odds Ratio  pvalue\n",
      "Time      0.999874       0.999880    0.999877     0.0\n",
      "V1        2.306871       2.573219    2.436408     0.0\n",
      "V2        0.607258       0.665317    0.635624     0.0\n",
      "V3        0.188703       0.214287    0.201088     0.0\n",
      "V4        1.099162       1.214710    1.155493     0.0\n",
      "V5        1.435225       1.575997    1.503965     0.0\n",
      "V6        0.644844       0.712365    0.677764     0.0\n",
      "V7        1.045965       1.165922    1.104316     0.0\n",
      "V8        0.647980       0.708401    0.677517     0.0\n",
      "V9        0.580064       0.686844    0.631200     0.0\n",
      "V10       0.618778       0.755967    0.683941     0.0\n",
      "V11       0.506517       0.579026    0.541560     0.0\n",
      "V12       1.060766       1.213565    1.134596     0.0\n",
      "V13       0.595111       0.681372    0.636782     0.0\n",
      "V14       0.467669       0.529359    0.497559     0.0\n",
      "V15       0.325146       0.382259    0.352548     0.0\n",
      "V16       0.726179       0.854508    0.787735     0.0\n",
      "V17       0.463798       0.527309    0.494535     0.0\n",
      "V20       0.461445       0.560874    0.508736     0.0\n",
      "V21       1.629043       1.912838    1.765246     0.0\n",
      "V22       3.435136       4.379916    3.878867     0.0\n",
      "V23       1.205547       1.491012    1.340704     0.0\n",
      "V25       0.117744       0.156375    0.135692     0.0\n",
      "V26       1.181497       1.565834    1.360157     0.0\n",
      "V27       0.398484       0.562859    0.473593     0.0\n"
     ]
    }
   ],
   "source": [
    "#Interpretiranje rezultata\n",
    "\n",
    "params = np.exp(result.params)\n",
    "conf = np.exp(result.conf_int())\n",
    "conf['OR'] = params\n",
    "pvalue=round(result.pvalues,3)\n",
    "conf['pvalue']=pvalue\n",
    "conf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\n",
    "print ((conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features=card[['Time','V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V20','V21', 'V22', 'V23', 'V25', 'V26', 'V27','Class']]\n",
    "x=new_features.iloc[:,:-1]\n",
    "y=new_features.iloc[:,-1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.3,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred=logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85295\n",
      "           1       0.75      0.74      0.74       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.87      0.87      0.87     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluacija modela\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Matrica konfuzije\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN=cm[0,0]\n",
    "TP=cm[1,1]\n",
    "FN=cm[1,0]\n",
    "FP=cm[0,1]\n",
    "sensitivity=TP/float(TP+FN)\n",
    "specificity=TN/float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =        0.9991105181231933 \n",
      " The Missclassification = 1-Accuracy =                   0.0008894818768067081 \n",
      " Sensitivity or True Positive Rate = TP/(TP+FN) =        0.7364864864864865 \n",
      " Specificity or True Negative Rate = TN/(TN+FP) =        0.9995662113840201 \n",
      " Positive Predictive value = TP/(TP+FP) =                0.7465753424657534 \n",
      " Negative predictive Value = TN/(TN+FN) =                0.9995427740717727 \n",
      " Positive Likelihood Ratio = Sensitivity/(1-Specificity) =  1697.8004017529713 \n",
      " Negative likelihood Ratio = (1-Sensitivity)/Specificity =  0.263627872283365\n"
     ]
    }
   ],
   "source": [
    "print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =       ',(TP+TN)/float(TP+TN+FP+FN),'\\n',\n",
    "\n",
    "'The Missclassification = 1-Accuracy =                  ',1-((TP+TN)/float(TP+TN+FP+FN)),'\\n',\n",
    "\n",
    "'Sensitivity or True Positive Rate = TP/(TP+FN) =       ',TP/float(TP+FN),'\\n',\n",
    "\n",
    "'Specificity or True Negative Rate = TN/(TN+FP) =       ',TN/float(TN+FP),'\\n',\n",
    "\n",
    "'Positive Predictive value = TP/(TP+FP) =               ',TP/float(TP+FP),'\\n',\n",
    "\n",
    "'Negative predictive Value = TN/(TN+FN) =               ',TN/float(TN+FN),'\\n',\n",
    "\n",
    "'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',sensitivity/(1-specificity),'\\n',\n",
    "      \n",
    "'Negative likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-sensitivity)/specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob of Not Fraud (0)</th>\n",
       "      <th>Prob of Fraud (1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999891</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.998148</td>\n",
       "      <td>0.001852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998522</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.945423</td>\n",
       "      <td>0.054577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prob of Not Fraud (0)  Prob of Fraud (1)\n",
       "0               0.999891           0.000109\n",
       "1               0.999998           0.000002\n",
       "2               0.998148           0.001852\n",
       "3               0.998522           0.001478\n",
       "4               0.945423           0.054577"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob=logreg.predict_proba(x_test)[:,:]\n",
    "y_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of Not Fraud (0)','Prob of Fraud (1)'])\n",
    "y_pred_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 0.0 threshold the Confusion Matrix is  \n",
      " [[    0 85295]\n",
      " [    0   148]] \n",
      " with 148 correct predictions and 0 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  1.0 Specificity:  0.0 \n",
      "\n",
      "\n",
      "\n",
      "With 0.1 threshold the Confusion Matrix is  \n",
      " [[85132   163]\n",
      " [   35   113]] \n",
      " with 85245 correct predictions and 35 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7635135135135135 Specificity:  0.9980889852863591 \n",
      "\n",
      "\n",
      "\n",
      "With 0.2 threshold the Confusion Matrix is  \n",
      " [[85220    75]\n",
      " [   36   112]] \n",
      " with 85332 correct predictions and 36 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7567567567567568 Specificity:  0.9991206987513922 \n",
      "\n",
      "\n",
      "\n",
      "With 0.3 threshold the Confusion Matrix is  \n",
      " [[85240    55]\n",
      " [   36   112]] \n",
      " with 85352 correct predictions and 36 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7567567567567568 Specificity:  0.9993551790843543 \n",
      "\n",
      "\n",
      "\n",
      "With 0.4 threshold the Confusion Matrix is  \n",
      " [[85249    46]\n",
      " [   37   111]] \n",
      " with 85360 correct predictions and 37 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.75 Specificity:  0.9994606952341872 \n",
      "\n",
      "\n",
      "\n",
      "With 0.5 threshold the Confusion Matrix is  \n",
      " [[85258    37]\n",
      " [   39   109]] \n",
      " with 85367 correct predictions and 39 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7364864864864865 Specificity:  0.9995662113840201 \n",
      "\n",
      "\n",
      "\n",
      "With 0.6 threshold the Confusion Matrix is  \n",
      " [[85262    33]\n",
      " [   39   109]] \n",
      " with 85371 correct predictions and 39 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7364864864864865 Specificity:  0.9996131074506126 \n",
      "\n",
      "\n",
      "\n",
      "With 0.7 threshold the Confusion Matrix is  \n",
      " [[85267    28]\n",
      " [   41   107]] \n",
      " with 85374 correct predictions and 41 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.722972972972973 Specificity:  0.9996717275338531 \n",
      "\n",
      "\n",
      "\n",
      "With 0.8 threshold the Confusion Matrix is  \n",
      " [[85269    26]\n",
      " [   45   103]] \n",
      " with 85372 correct predictions and 45 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.6959459459459459 Specificity:  0.9996951755671493 \n",
      "\n",
      "\n",
      "\n",
      "With 0.9 threshold the Confusion Matrix is  \n",
      " [[85275    20]\n",
      " [   49    99]] \n",
      " with 85374 correct predictions and 49 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.668918918918919 Specificity:  0.999765519667038 \n",
      "\n",
      "\n",
      "\n",
      "With 1.0 threshold the Confusion Matrix is  \n",
      " [[85295     0]\n",
      " [  148     0]] \n",
      " with 85295 correct predictions and 148 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.0 Specificity:  1.0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "for i in range(0,11):\n",
    "    cm2=0\n",
    "    y_pred_prob_yes=logreg.predict_proba(x_test)\n",
    "    y_pred2=binarize(y_pred_prob_yes,i/10)[:,1]\n",
    "    cm2=confusion_matrix(y_test,y_pred2)\n",
    "    print ('With',i/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n",
    "            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n",
    "          'Sensitivity: ',cm2[1,1]/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for Fraud classifier')\n",
    "plt.xlabel('False positive rate (1-Specificity)')\n",
    "plt.ylabel('True positive rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9393172027763739"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_pred_prob_yes[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "     Amount  Class  \n",
       "0  0.005824      0  \n",
       "1  0.000105      0  \n",
       "2  0.014739      0  \n",
       "3  0.004807      0  \n",
       "4  0.002724      0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NORMALIZOVANI PODACI\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "norm = pd.read_csv(r'C:\\Users\\pc\\OneDrive\\Desktop\\folderi\\FON\\mfs\\Kod\\normalizovano.csv')\n",
    "norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
    "\n",
    "n_X1 = norm[estimators]\n",
    "n_y = norm['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_col=n_X1.columns[:-1]\n",
    "n_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003914\n",
      "         Iterations 14\n"
     ]
    }
   ],
   "source": [
    "n_X = sm.add_constant(n_X1)\n",
    "n_reg_logit = sm.Logit(n_y,n_X)\n",
    "n_results_logit = n_reg_logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>284807</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>284776</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    30</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 22 May 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.6922</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:26:15</td>     <th>  Log-Likelihood:    </th> <td> -1114.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -3621.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -8.3917</td> <td>    0.249</td> <td>  -33.652</td> <td> 0.000</td> <td>   -8.880</td> <td>   -7.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th>   <td>-3.742e-06</td> <td> 2.26e-06</td> <td>   -1.659</td> <td> 0.097</td> <td>-8.16e-06</td> <td> 6.79e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>     <td>    0.0960</td> <td>    0.042</td> <td>    2.264</td> <td> 0.024</td> <td>    0.013</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>     <td>    0.0094</td> <td>    0.058</td> <td>    0.161</td> <td> 0.872</td> <td>   -0.104</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>     <td>   -0.0079</td> <td>    0.053</td> <td>   -0.149</td> <td> 0.881</td> <td>   -0.112</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>     <td>    0.6986</td> <td>    0.074</td> <td>    9.454</td> <td> 0.000</td> <td>    0.554</td> <td>    0.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>     <td>    0.1295</td> <td>    0.067</td> <td>    1.944</td> <td> 0.052</td> <td>   -0.001</td> <td>    0.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>     <td>   -0.1198</td> <td>    0.074</td> <td>   -1.626</td> <td> 0.104</td> <td>   -0.264</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>     <td>   -0.0969</td> <td>    0.067</td> <td>   -1.453</td> <td> 0.146</td> <td>   -0.228</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>     <td>   -0.1739</td> <td>    0.030</td> <td>   -5.711</td> <td> 0.000</td> <td>   -0.234</td> <td>   -0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>     <td>   -0.2843</td> <td>    0.111</td> <td>   -2.561</td> <td> 0.010</td> <td>   -0.502</td> <td>   -0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>    <td>   -0.8176</td> <td>    0.097</td> <td>   -8.432</td> <td> 0.000</td> <td>   -1.008</td> <td>   -0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>    <td>   -0.0621</td> <td>    0.081</td> <td>   -0.762</td> <td> 0.446</td> <td>   -0.222</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>    <td>    0.0909</td> <td>    0.087</td> <td>    1.045</td> <td> 0.296</td> <td>   -0.080</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>    <td>   -0.3312</td> <td>    0.082</td> <td>   -4.058</td> <td> 0.000</td> <td>   -0.491</td> <td>   -0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>    <td>   -0.5571</td> <td>    0.062</td> <td>   -8.949</td> <td> 0.000</td> <td>   -0.679</td> <td>   -0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>    <td>   -0.1141</td> <td>    0.086</td> <td>   -1.330</td> <td> 0.183</td> <td>   -0.282</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>    <td>   -0.1908</td> <td>    0.125</td> <td>   -1.526</td> <td> 0.127</td> <td>   -0.436</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>    <td>   -0.0216</td> <td>    0.070</td> <td>   -0.309</td> <td> 0.757</td> <td>   -0.159</td> <td>    0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>    <td>   -0.0131</td> <td>    0.129</td> <td>   -0.102</td> <td> 0.919</td> <td>   -0.266</td> <td>    0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>    <td>    0.0963</td> <td>    0.097</td> <td>    0.993</td> <td> 0.321</td> <td>   -0.094</td> <td>    0.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>    <td>   -0.4582</td> <td>    0.082</td> <td>   -5.607</td> <td> 0.000</td> <td>   -0.618</td> <td>   -0.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>    <td>    0.3898</td> <td>    0.060</td> <td>    6.494</td> <td> 0.000</td> <td>    0.272</td> <td>    0.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>    <td>    0.6297</td> <td>    0.134</td> <td>    4.707</td> <td> 0.000</td> <td>    0.367</td> <td>    0.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>    <td>   -0.0951</td> <td>    0.058</td> <td>   -1.629</td> <td> 0.103</td> <td>   -0.209</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>    <td>    0.1289</td> <td>    0.147</td> <td>    0.874</td> <td> 0.382</td> <td>   -0.160</td> <td>    0.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>    <td>   -0.0761</td> <td>    0.131</td> <td>   -0.582</td> <td> 0.560</td> <td>   -0.332</td> <td>    0.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>    <td>    0.0195</td> <td>    0.190</td> <td>    0.103</td> <td> 0.918</td> <td>   -0.352</td> <td>    0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>    <td>   -0.8188</td> <td>    0.122</td> <td>   -6.686</td> <td> 0.000</td> <td>   -1.059</td> <td>   -0.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>    <td>   -0.2937</td> <td>    0.088</td> <td>   -3.332</td> <td> 0.001</td> <td>   -0.467</td> <td>   -0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th> <td>   23.5301</td> <td>    9.609</td> <td>    2.449</td> <td> 0.014</td> <td>    4.696</td> <td>   42.364</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.31 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               284807\n",
       "Model:                          Logit   Df Residuals:                   284776\n",
       "Method:                           MLE   Df Model:                           30\n",
       "Date:                Sat, 22 May 2021   Pseudo R-squ.:                  0.6922\n",
       "Time:                        16:26:15   Log-Likelihood:                -1114.8\n",
       "converged:                       True   LL-Null:                       -3621.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -8.3917      0.249    -33.652      0.000      -8.880      -7.903\n",
       "Time       -3.742e-06   2.26e-06     -1.659      0.097   -8.16e-06    6.79e-07\n",
       "V1             0.0960      0.042      2.264      0.024       0.013       0.179\n",
       "V2             0.0094      0.058      0.161      0.872      -0.104       0.123\n",
       "V3            -0.0079      0.053     -0.149      0.881      -0.112       0.096\n",
       "V4             0.6986      0.074      9.454      0.000       0.554       0.843\n",
       "V5             0.1295      0.067      1.944      0.052      -0.001       0.260\n",
       "V6            -0.1198      0.074     -1.626      0.104      -0.264       0.025\n",
       "V7            -0.0969      0.067     -1.453      0.146      -0.228       0.034\n",
       "V8            -0.1739      0.030     -5.711      0.000      -0.234      -0.114\n",
       "V9            -0.2843      0.111     -2.561      0.010      -0.502      -0.067\n",
       "V10           -0.8176      0.097     -8.432      0.000      -1.008      -0.628\n",
       "V11           -0.0621      0.081     -0.762      0.446      -0.222       0.098\n",
       "V12            0.0909      0.087      1.045      0.296      -0.080       0.261\n",
       "V13           -0.3312      0.082     -4.058      0.000      -0.491      -0.171\n",
       "V14           -0.5571      0.062     -8.949      0.000      -0.679      -0.435\n",
       "V15           -0.1141      0.086     -1.330      0.183      -0.282       0.054\n",
       "V16           -0.1908      0.125     -1.526      0.127      -0.436       0.054\n",
       "V17           -0.0216      0.070     -0.309      0.757      -0.159       0.116\n",
       "V18           -0.0131      0.129     -0.102      0.919      -0.266       0.240\n",
       "V19            0.0963      0.097      0.993      0.321      -0.094       0.286\n",
       "V20           -0.4582      0.082     -5.607      0.000      -0.618      -0.298\n",
       "V21            0.3898      0.060      6.494      0.000       0.272       0.507\n",
       "V22            0.6297      0.134      4.707      0.000       0.367       0.892\n",
       "V23           -0.0951      0.058     -1.629      0.103      -0.209       0.019\n",
       "V24            0.1289      0.147      0.874      0.382      -0.160       0.418\n",
       "V25           -0.0761      0.131     -0.582      0.560      -0.332       0.180\n",
       "V26            0.0195      0.190      0.103      0.918      -0.352       0.392\n",
       "V27           -0.8188      0.122     -6.686      0.000      -1.059      -0.579\n",
       "V28           -0.2937      0.088     -3.332      0.001      -0.467      -0.121\n",
       "Amount        23.5301      9.609      2.449      0.014       4.696      42.364\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.31 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_results_logit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_back_feature_elem (data_frame,dep_var,col_list):\n",
    "    \n",
    "\n",
    "    while len(col_list)>0 :\n",
    "        n_model=sm.Logit(dep_var,data_frame[col_list])\n",
    "        n_result=n_model.fit(disp=0)\n",
    "        n_largest_pvalue=round(n_result.pvalues,3).nlargest(1)\n",
    "        if n_largest_pvalue[0]<(0.0001):\n",
    "            return n_result\n",
    "            break\n",
    "        else:\n",
    "            col_list=col_list.drop(n_largest_pvalue.index)\n",
    "\n",
    "n_result=n_back_feature_elem(n_X,norm.Class,n_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>  <td>284807</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>284782</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    24</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sat, 22 May 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.06233</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>16:32:15</td>     <th>  Log-Likelihood:    </th> <td> -3395.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -3621.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.919e-80</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th> <td>   -0.0001</td> <td> 1.43e-06</td> <td>  -86.049</td> <td> 0.000</td> <td>   -0.000</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>   <td>    0.8905</td> <td>    0.028</td> <td>   31.948</td> <td> 0.000</td> <td>    0.836</td> <td>    0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>   <td>   -0.4531</td> <td>    0.023</td> <td>  -19.454</td> <td> 0.000</td> <td>   -0.499</td> <td>   -0.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>   <td>   -1.6040</td> <td>    0.032</td> <td>  -49.454</td> <td> 0.000</td> <td>   -1.668</td> <td>   -1.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>   <td>    0.1445</td> <td>    0.025</td> <td>    5.668</td> <td> 0.000</td> <td>    0.095</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>   <td>    0.4081</td> <td>    0.024</td> <td>   17.097</td> <td> 0.000</td> <td>    0.361</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>   <td>   -0.3890</td> <td>    0.025</td> <td>  -15.311</td> <td> 0.000</td> <td>   -0.439</td> <td>   -0.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>   <td>    0.0992</td> <td>    0.028</td> <td>    3.582</td> <td> 0.000</td> <td>    0.045</td> <td>    0.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>   <td>   -0.3893</td> <td>    0.023</td> <td>  -17.118</td> <td> 0.000</td> <td>   -0.434</td> <td>   -0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>   <td>   -0.4601</td> <td>    0.043</td> <td>  -10.675</td> <td> 0.000</td> <td>   -0.545</td> <td>   -0.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>  <td>   -0.3799</td> <td>    0.051</td> <td>   -7.436</td> <td> 0.000</td> <td>   -0.480</td> <td>   -0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>  <td>   -0.6133</td> <td>    0.034</td> <td>  -17.969</td> <td> 0.000</td> <td>   -0.680</td> <td>   -0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>  <td>    0.1263</td> <td>    0.034</td> <td>    3.678</td> <td> 0.000</td> <td>    0.059</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>  <td>   -0.4513</td> <td>    0.035</td> <td>  -13.070</td> <td> 0.000</td> <td>   -0.519</td> <td>   -0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>  <td>   -0.6980</td> <td>    0.032</td> <td>  -22.083</td> <td> 0.000</td> <td>   -0.760</td> <td>   -0.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>  <td>   -1.0426</td> <td>    0.041</td> <td>  -25.255</td> <td> 0.000</td> <td>   -1.123</td> <td>   -0.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>  <td>   -0.2386</td> <td>    0.042</td> <td>   -5.747</td> <td> 0.000</td> <td>   -0.320</td> <td>   -0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>  <td>   -0.7041</td> <td>    0.033</td> <td>  -21.507</td> <td> 0.000</td> <td>   -0.768</td> <td>   -0.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>  <td>   -0.6758</td> <td>    0.050</td> <td>  -13.576</td> <td> 0.000</td> <td>   -0.773</td> <td>   -0.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>  <td>    0.5683</td> <td>    0.041</td> <td>   13.871</td> <td> 0.000</td> <td>    0.488</td> <td>    0.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>  <td>    1.3555</td> <td>    0.062</td> <td>   21.869</td> <td> 0.000</td> <td>    1.234</td> <td>    1.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>  <td>    0.2932</td> <td>    0.054</td> <td>    5.408</td> <td> 0.000</td> <td>    0.187</td> <td>    0.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>  <td>   -1.9974</td> <td>    0.072</td> <td>  -27.593</td> <td> 0.000</td> <td>   -2.139</td> <td>   -1.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>  <td>    0.3076</td> <td>    0.072</td> <td>    4.281</td> <td> 0.000</td> <td>    0.167</td> <td>    0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>  <td>   -0.7474</td> <td>    0.088</td> <td>   -8.483</td> <td> 0.000</td> <td>   -0.920</td> <td>   -0.575</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.68 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               284807\n",
       "Model:                          Logit   Df Residuals:                   284782\n",
       "Method:                           MLE   Df Model:                           24\n",
       "Date:                Sat, 22 May 2021   Pseudo R-squ.:                 0.06233\n",
       "Time:                        16:32:15   Log-Likelihood:                -3395.5\n",
       "converged:                       True   LL-Null:                       -3621.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.919e-80\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Time          -0.0001   1.43e-06    -86.049      0.000      -0.000      -0.000\n",
       "V1             0.8905      0.028     31.948      0.000       0.836       0.945\n",
       "V2            -0.4531      0.023    -19.454      0.000      -0.499      -0.407\n",
       "V3            -1.6040      0.032    -49.454      0.000      -1.668      -1.540\n",
       "V4             0.1445      0.025      5.668      0.000       0.095       0.195\n",
       "V5             0.4081      0.024     17.097      0.000       0.361       0.455\n",
       "V6            -0.3890      0.025    -15.311      0.000      -0.439      -0.339\n",
       "V7             0.0992      0.028      3.582      0.000       0.045       0.154\n",
       "V8            -0.3893      0.023    -17.118      0.000      -0.434      -0.345\n",
       "V9            -0.4601      0.043    -10.675      0.000      -0.545      -0.376\n",
       "V10           -0.3799      0.051     -7.436      0.000      -0.480      -0.280\n",
       "V11           -0.6133      0.034    -17.969      0.000      -0.680      -0.546\n",
       "V12            0.1263      0.034      3.678      0.000       0.059       0.194\n",
       "V13           -0.4513      0.035    -13.070      0.000      -0.519      -0.384\n",
       "V14           -0.6980      0.032    -22.083      0.000      -0.760      -0.636\n",
       "V15           -1.0426      0.041    -25.255      0.000      -1.123      -0.962\n",
       "V16           -0.2386      0.042     -5.747      0.000      -0.320      -0.157\n",
       "V17           -0.7041      0.033    -21.507      0.000      -0.768      -0.640\n",
       "V20           -0.6758      0.050    -13.576      0.000      -0.773      -0.578\n",
       "V21            0.5683      0.041     13.871      0.000       0.488       0.649\n",
       "V22            1.3555      0.062     21.869      0.000       1.234       1.477\n",
       "V23            0.2932      0.054      5.408      0.000       0.187       0.399\n",
       "V25           -1.9974      0.072    -27.593      0.000      -2.139      -1.855\n",
       "V26            0.3076      0.072      4.281      0.000       0.167       0.448\n",
       "V27           -0.7474      0.088     -8.483      0.000      -0.920      -0.575\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.68 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_result.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CI 95%(2.5%)  CI 95%(97.5%)  Odds Ratio  pvalue\n",
      "Time      0.999874       0.999880    0.999877     0.0\n",
      "V1        2.306871       2.573219    2.436408     0.0\n",
      "V2        0.607258       0.665317    0.635624     0.0\n",
      "V3        0.188703       0.214287    0.201088     0.0\n",
      "V4        1.099162       1.214710    1.155493     0.0\n",
      "V5        1.435225       1.575997    1.503965     0.0\n",
      "V6        0.644844       0.712365    0.677764     0.0\n",
      "V7        1.045965       1.165922    1.104316     0.0\n",
      "V8        0.647980       0.708401    0.677517     0.0\n",
      "V9        0.580064       0.686844    0.631200     0.0\n",
      "V10       0.618778       0.755967    0.683941     0.0\n",
      "V11       0.506517       0.579026    0.541560     0.0\n",
      "V12       1.060766       1.213565    1.134596     0.0\n",
      "V13       0.595111       0.681372    0.636782     0.0\n",
      "V14       0.467669       0.529359    0.497559     0.0\n",
      "V15       0.325146       0.382259    0.352548     0.0\n",
      "V16       0.726179       0.854508    0.787735     0.0\n",
      "V17       0.463798       0.527309    0.494535     0.0\n",
      "V20       0.461445       0.560874    0.508736     0.0\n",
      "V21       1.629043       1.912838    1.765246     0.0\n",
      "V22       3.435136       4.379916    3.878867     0.0\n",
      "V23       1.205547       1.491012    1.340704     0.0\n",
      "V25       0.117744       0.156375    0.135692     0.0\n",
      "V26       1.181497       1.565834    1.360157     0.0\n",
      "V27       0.398484       0.562859    0.473593     0.0\n"
     ]
    }
   ],
   "source": [
    "#Interpretiranje rezultata\n",
    "\n",
    "n_params = np.exp(n_result.params)\n",
    "n_conf = np.exp(n_result.conf_int())\n",
    "n_conf['OR'] = n_params\n",
    "n_pvalue=round(n_result.pvalues,3)\n",
    "n_conf['pvalue']=n_pvalue\n",
    "n_conf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\n",
    "print ((n_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new_features=norm[['Time','V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V20','V21', 'V22', 'V23', 'V25', 'V26', 'V27','Class']]\n",
    "n_x=n_new_features.iloc[:,:-1]\n",
    "n_y=n_new_features.iloc[:,-1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "n_x_train,n_x_test,n_y_train,n_y_test=train_test_split(n_x,n_y,test_size=.3,stratify=n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "n_logreg=LogisticRegression()\n",
    "n_logreg.fit(n_x_train,n_y_train)\n",
    "n_y_pred=n_logreg.predict(n_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85295\n",
      "           1       0.73      0.72      0.73       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.86      0.86      0.86     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluacija modela\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(n_y_test,n_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Matrica konfuzije\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "n_cm=confusion_matrix(n_y_test,n_y_pred)\n",
    "n_conf_matrix=pd.DataFrame(data=n_cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(n_conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_TN=n_cm[0,0]\n",
    "n_TP=n_cm[1,1]\n",
    "n_FN=n_cm[1,0]\n",
    "n_FP=n_cm[0,1]\n",
    "n_sensitivity=n_TP/float(n_TP+n_FN)\n",
    "n_specificity=n_TN/float(n_TN+n_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =        0.9990519995786665 \n",
      " The Missclassification = 1-Accuracy =                   0.0009480004213334725 \n",
      " Sensitivity or True Positive Rate = TP/(TP+FN) =        0.722972972972973 \n",
      " Specificity or True Negative Rate = TN/(TN+FP) =        0.9995310393340758 \n",
      " Positive Predictive value = TP/(TP+FP) =                0.7278911564625851 \n",
      " Negative predictive Value = TN/(TN+FN) =                0.999519320952917 \n",
      " Positive Likelihood Ratio = Sensitivity/(1-Specificity) =  1541.6494932431717 \n",
      " Negative likelihood Ratio = (1-Sensitivity)/Specificity =  0.27715700275960664\n"
     ]
    }
   ],
   "source": [
    "print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =       ',(n_TP+n_TN)/float(n_TP+n_TN+n_FP+n_FN),'\\n',\n",
    "\n",
    "'The Missclassification = 1-Accuracy =                  ',1-((n_TP+n_TN)/float(n_TP+n_TN+n_FP+n_FN)),'\\n',\n",
    "\n",
    "'Sensitivity or True Positive Rate = TP/(TP+FN) =       ',n_TP/float(n_TP+n_FN),'\\n',\n",
    "\n",
    "'Specificity or True Negative Rate = TN/(TN+FP) =       ',n_TN/float(n_TN+n_FP),'\\n',\n",
    "\n",
    "'Positive Predictive value = TP/(TP+FP) =               ',n_TP/float(n_TP+n_FP),'\\n',\n",
    "\n",
    "'Negative predictive Value = TN/(TN+FN) =               ',n_TN/float(n_TN+n_FN),'\\n',\n",
    "\n",
    "'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',n_sensitivity/(1-n_specificity),'\\n',\n",
    "      \n",
    "'Negative likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-n_sensitivity)/n_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob of Not Fraud (0)</th>\n",
       "      <th>Prob of Fraud (1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999986</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997858</td>\n",
       "      <td>0.002142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999522</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prob of Not Fraud (0)  Prob of Fraud (1)\n",
       "0               0.999946           0.000054\n",
       "1               0.999959           0.000041\n",
       "2               0.999986           0.000014\n",
       "3               0.997858           0.002142\n",
       "4               0.999522           0.000478"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_y_pred_prob=n_logreg.predict_proba(n_x_test)[:,:]\n",
    "n_y_pred_prob_df=pd.DataFrame(data=n_y_pred_prob, columns=['Prob of Not Fraud (0)','Prob of Fraud (1)'])\n",
    "n_y_pred_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 0.0 threshold the Confusion Matrix is  \n",
      " [[    0 85295]\n",
      " [    0   148]] \n",
      " with 148 correct predictions and 0 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  1.0 Specificity:  0.0 \n",
      "\n",
      "\n",
      "\n",
      "With 0.1 threshold the Confusion Matrix is  \n",
      " [[85085   210]\n",
      " [   30   118]] \n",
      " with 85203 correct predictions and 30 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7972972972972973 Specificity:  0.9975379565038982 \n",
      "\n",
      "\n",
      "\n",
      "With 0.2 threshold the Confusion Matrix is  \n",
      " [[85205    90]\n",
      " [   34   114]] \n",
      " with 85319 correct predictions and 34 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7702702702702703 Specificity:  0.9989448385016707 \n",
      "\n",
      "\n",
      "\n",
      "With 0.3 threshold the Confusion Matrix is  \n",
      " [[85239    56]\n",
      " [   36   112]] \n",
      " with 85351 correct predictions and 36 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7567567567567568 Specificity:  0.9993434550677062 \n",
      "\n",
      "\n",
      "\n",
      "With 0.4 threshold the Confusion Matrix is  \n",
      " [[85247    48]\n",
      " [   38   110]] \n",
      " with 85357 correct predictions and 38 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7432432432432432 Specificity:  0.999437247200891 \n",
      "\n",
      "\n",
      "\n",
      "With 0.5 threshold the Confusion Matrix is  \n",
      " [[85255    40]\n",
      " [   41   107]] \n",
      " with 85362 correct predictions and 41 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.722972972972973 Specificity:  0.9995310393340758 \n",
      "\n",
      "\n",
      "\n",
      "With 0.6 threshold the Confusion Matrix is  \n",
      " [[85260    35]\n",
      " [   44   104]] \n",
      " with 85364 correct predictions and 44 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.7027027027027027 Specificity:  0.9995896594173164 \n",
      "\n",
      "\n",
      "\n",
      "With 0.7 threshold the Confusion Matrix is  \n",
      " [[85263    32]\n",
      " [   50    98]] \n",
      " with 85361 correct predictions and 50 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.6621621621621622 Specificity:  0.9996248314672607 \n",
      "\n",
      "\n",
      "\n",
      "With 0.8 threshold the Confusion Matrix is  \n",
      " [[85271    24]\n",
      " [   52    96]] \n",
      " with 85367 correct predictions and 52 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.6486486486486487 Specificity:  0.9997186236004455 \n",
      "\n",
      "\n",
      "\n",
      "With 0.9 threshold the Confusion Matrix is  \n",
      " [[85279    16]\n",
      " [   60    88]] \n",
      " with 85367 correct predictions and 60 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.5945945945945946 Specificity:  0.9998124157336303 \n",
      "\n",
      "\n",
      "\n",
      "With 1.0 threshold the Confusion Matrix is  \n",
      " [[85295     0]\n",
      " [  148     0]] \n",
      " with 85295 correct predictions and 148 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.0 Specificity:  1.0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "for i in range(0,11):\n",
    "    n_cm2=0\n",
    "    n_y_pred_prob_yes=n_logreg.predict_proba(n_x_test)\n",
    "    n_y_pred2=binarize(n_y_pred_prob_yes,i/10)[:,1]\n",
    "    n_cm2=confusion_matrix(n_y_test,n_y_pred2)\n",
    "    print ('With',i/10,'threshold the Confusion Matrix is ','\\n',n_cm2,'\\n',\n",
    "            'with',n_cm2[0,0]+n_cm2[1,1],'correct predictions and',n_cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n",
    "          'Sensitivity: ',n_cm2[1,1]/(float(n_cm2[1,1]+n_cm2[1,0])),'Specificity: ',n_cm2[0,0]/(float(n_cm2[0,0]+n_cm2[0,1])),'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "n_fpr, n_tpr, thresholds = roc_curve(n_y_test, n_y_pred_prob_yes[:,1])\n",
    "plt.plot(n_fpr,n_tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for Fraud classifier - Normalized')\n",
    "plt.xlabel('False positive rate (1-Specificity)')\n",
    "plt.ylabel('True positive rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466733894924293"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(n_y_test,n_y_pred_prob_yes[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62    0.0  \n",
       "1    2.69    0.0  \n",
       "2  378.66    0.0  \n",
       "3  123.50    0.0  \n",
       "4   69.99    0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BALANSIRANI PODACI\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "bal = pd.read_csv(r'C:\\Users\\pc\\Downloads\\smote_creditcard.csv')\n",
    "bal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_estimators=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
    "\n",
    "b_X1 = bal[b_estimators]\n",
    "b_y = bal['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_col=b_X1.columns[:-1]\n",
    "b_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.061623\n",
      "         Iterations 15\n"
     ]
    }
   ],
   "source": [
    "b_X = sm.add_constant(b_X1)\n",
    "b_reg_logit = sm.Logit(b_y,b_X)\n",
    "b_results_logit = b_reg_logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>   <td>568630</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>568599</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    30</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 26 May 2021</td> <th>  Pseudo R-squ.:     </th>   <td>0.9111</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>23:10:55</td>     <th>  Log-Likelihood:    </th>  <td> -35041.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-3.9414e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -5.7778</td> <td>    0.054</td> <td> -107.365</td> <td> 0.000</td> <td>   -5.883</td> <td>   -5.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th>   <td>-1.015e-05</td> <td> 3.39e-07</td> <td>  -29.909</td> <td> 0.000</td> <td>-1.08e-05</td> <td>-9.49e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>     <td>    1.4536</td> <td>    0.023</td> <td>   63.741</td> <td> 0.000</td> <td>    1.409</td> <td>    1.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>     <td>    1.2828</td> <td>    0.040</td> <td>   32.443</td> <td> 0.000</td> <td>    1.205</td> <td>    1.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>     <td>    0.7809</td> <td>    0.019</td> <td>   42.169</td> <td> 0.000</td> <td>    0.745</td> <td>    0.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>     <td>    0.7863</td> <td>    0.013</td> <td>   61.174</td> <td> 0.000</td> <td>    0.761</td> <td>    0.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>     <td>    1.4624</td> <td>    0.028</td> <td>   51.835</td> <td> 0.000</td> <td>    1.407</td> <td>    1.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>     <td>   -0.9020</td> <td>    0.019</td> <td>  -47.157</td> <td> 0.000</td> <td>   -0.940</td> <td>   -0.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>     <td>   -1.4123</td> <td>    0.036</td> <td>  -39.718</td> <td> 0.000</td> <td>   -1.482</td> <td>   -1.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>     <td>   -0.4989</td> <td>    0.012</td> <td>  -40.309</td> <td> 0.000</td> <td>   -0.523</td> <td>   -0.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>     <td>   -0.5752</td> <td>    0.018</td> <td>  -32.223</td> <td> 0.000</td> <td>   -0.610</td> <td>   -0.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>    <td>   -1.1835</td> <td>    0.023</td> <td>  -50.643</td> <td> 0.000</td> <td>   -1.229</td> <td>   -1.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>    <td>    0.8999</td> <td>    0.015</td> <td>   61.583</td> <td> 0.000</td> <td>    0.871</td> <td>    0.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>    <td>   -1.8083</td> <td>    0.020</td> <td>  -90.655</td> <td> 0.000</td> <td>   -1.847</td> <td>   -1.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>    <td>   -0.5546</td> <td>    0.011</td> <td>  -50.733</td> <td> 0.000</td> <td>   -0.576</td> <td>   -0.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>    <td>   -2.2760</td> <td>    0.022</td> <td> -101.593</td> <td> 0.000</td> <td>   -2.320</td> <td>   -2.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>    <td>   -0.2230</td> <td>    0.012</td> <td>  -18.253</td> <td> 0.000</td> <td>   -0.247</td> <td>   -0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>    <td>   -1.1337</td> <td>    0.023</td> <td>  -50.194</td> <td> 0.000</td> <td>   -1.178</td> <td>   -1.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>    <td>   -1.9664</td> <td>    0.027</td> <td>  -72.820</td> <td> 0.000</td> <td>   -2.019</td> <td>   -1.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>    <td>   -0.7489</td> <td>    0.020</td> <td>  -37.065</td> <td> 0.000</td> <td>   -0.789</td> <td>   -0.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>    <td>    0.7323</td> <td>    0.017</td> <td>   43.866</td> <td> 0.000</td> <td>    0.700</td> <td>    0.765</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>    <td>   -1.4488</td> <td>    0.036</td> <td>  -39.699</td> <td> 0.000</td> <td>   -1.520</td> <td>   -1.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>    <td>   -0.0147</td> <td>    0.015</td> <td>   -1.003</td> <td> 0.316</td> <td>   -0.043</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>    <td>    1.0613</td> <td>    0.021</td> <td>   50.363</td> <td> 0.000</td> <td>    1.020</td> <td>    1.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>    <td>    1.3350</td> <td>    0.035</td> <td>   38.147</td> <td> 0.000</td> <td>    1.266</td> <td>    1.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>    <td>    0.0828</td> <td>    0.022</td> <td>    3.742</td> <td> 0.000</td> <td>    0.039</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>    <td>    0.1443</td> <td>    0.024</td> <td>    5.923</td> <td> 0.000</td> <td>    0.097</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>    <td>   -0.2182</td> <td>    0.026</td> <td>   -8.296</td> <td> 0.000</td> <td>   -0.270</td> <td>   -0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>    <td>   -0.0524</td> <td>    0.042</td> <td>   -1.250</td> <td> 0.211</td> <td>   -0.135</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>    <td>    1.6534</td> <td>    0.048</td> <td>   34.701</td> <td> 0.000</td> <td>    1.560</td> <td>    1.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th> <td>    0.0165</td> <td>    0.000</td> <td>   43.774</td> <td> 0.000</td> <td>    0.016</td> <td>    0.017</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.48 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               568630\n",
       "Model:                          Logit   Df Residuals:                   568599\n",
       "Method:                           MLE   Df Model:                           30\n",
       "Date:                Wed, 26 May 2021   Pseudo R-squ.:                  0.9111\n",
       "Time:                        23:10:55   Log-Likelihood:                -35041.\n",
       "converged:                       True   LL-Null:                   -3.9414e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -5.7778      0.054   -107.365      0.000      -5.883      -5.672\n",
       "Time       -1.015e-05   3.39e-07    -29.909      0.000   -1.08e-05   -9.49e-06\n",
       "V1             1.4536      0.023     63.741      0.000       1.409       1.498\n",
       "V2             1.2828      0.040     32.443      0.000       1.205       1.360\n",
       "V3             0.7809      0.019     42.169      0.000       0.745       0.817\n",
       "V4             0.7863      0.013     61.174      0.000       0.761       0.811\n",
       "V5             1.4624      0.028     51.835      0.000       1.407       1.518\n",
       "V6            -0.9020      0.019    -47.157      0.000      -0.940      -0.865\n",
       "V7            -1.4123      0.036    -39.718      0.000      -1.482      -1.343\n",
       "V8            -0.4989      0.012    -40.309      0.000      -0.523      -0.475\n",
       "V9            -0.5752      0.018    -32.223      0.000      -0.610      -0.540\n",
       "V10           -1.1835      0.023    -50.643      0.000      -1.229      -1.138\n",
       "V11            0.8999      0.015     61.583      0.000       0.871       0.929\n",
       "V12           -1.8083      0.020    -90.655      0.000      -1.847      -1.769\n",
       "V13           -0.5546      0.011    -50.733      0.000      -0.576      -0.533\n",
       "V14           -2.2760      0.022   -101.593      0.000      -2.320      -2.232\n",
       "V15           -0.2230      0.012    -18.253      0.000      -0.247      -0.199\n",
       "V16           -1.1337      0.023    -50.194      0.000      -1.178      -1.089\n",
       "V17           -1.9664      0.027    -72.820      0.000      -2.019      -1.914\n",
       "V18           -0.7489      0.020    -37.065      0.000      -0.789      -0.709\n",
       "V19            0.7323      0.017     43.866      0.000       0.700       0.765\n",
       "V20           -1.4488      0.036    -39.699      0.000      -1.520      -1.377\n",
       "V21           -0.0147      0.015     -1.003      0.316      -0.043       0.014\n",
       "V22            1.0613      0.021     50.363      0.000       1.020       1.103\n",
       "V23            1.3350      0.035     38.147      0.000       1.266       1.404\n",
       "V24            0.0828      0.022      3.742      0.000       0.039       0.126\n",
       "V25            0.1443      0.024      5.923      0.000       0.097       0.192\n",
       "V26           -0.2182      0.026     -8.296      0.000      -0.270      -0.167\n",
       "V27           -0.0524      0.042     -1.250      0.211      -0.135       0.030\n",
       "V28            1.6534      0.048     34.701      0.000       1.560       1.747\n",
       "Amount         0.0165      0.000     43.774      0.000       0.016       0.017\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.48 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_results_logit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_back_feature_elem (data_frame,dep_var,col_list):\n",
    "\n",
    "    while len(col_list)>0 :\n",
    "        b_model=sm.Logit(dep_var,data_frame[col_list])\n",
    "        b_result=b_model.fit(disp=0)\n",
    "        b_largest_pvalue=round(b_result.pvalues,3).nlargest(1)\n",
    "        if b_largest_pvalue[0]<(0.0001):\n",
    "            return b_result\n",
    "            break\n",
    "        else:\n",
    "            col_list=col_list.drop(b_largest_pvalue.index)\n",
    "\n",
    "b_result=b_back_feature_elem(b_X,bal.Class,b_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>   <td>568630</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>568601</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    28</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 26 May 2021</td> <th>  Pseudo R-squ.:     </th>   <td>0.8859</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>23:17:30</td>     <th>  Log-Likelihood:    </th>  <td> -44982.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-3.9414e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th> <td>-4.918e-05</td> <td> 2.17e-07</td> <td> -226.539</td> <td> 0.000</td> <td>-4.96e-05</td> <td>-4.88e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>   <td>    0.3778</td> <td>    0.011</td> <td>   34.073</td> <td> 0.000</td> <td>    0.356</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>   <td>   -0.1572</td> <td>    0.010</td> <td>  -16.366</td> <td> 0.000</td> <td>   -0.176</td> <td>   -0.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>   <td>   -0.9427</td> <td>    0.010</td> <td>  -90.684</td> <td> 0.000</td> <td>   -0.963</td> <td>   -0.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>   <td>    0.9221</td> <td>    0.008</td> <td>  116.435</td> <td> 0.000</td> <td>    0.907</td> <td>    0.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>   <td>    0.2028</td> <td>    0.010</td> <td>   20.666</td> <td> 0.000</td> <td>    0.184</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>   <td>   -0.3329</td> <td>    0.008</td> <td>  -42.561</td> <td> 0.000</td> <td>   -0.348</td> <td>   -0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>   <td>   -0.2693</td> <td>    0.012</td> <td>  -21.929</td> <td> 0.000</td> <td>   -0.293</td> <td>   -0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>   <td>   -0.5386</td> <td>    0.010</td> <td>  -52.693</td> <td> 0.000</td> <td>   -0.559</td> <td>   -0.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>   <td>   -0.6680</td> <td>    0.013</td> <td>  -51.931</td> <td> 0.000</td> <td>   -0.693</td> <td>   -0.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>  <td>   -1.0888</td> <td>    0.017</td> <td>  -62.842</td> <td> 0.000</td> <td>   -1.123</td> <td>   -1.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>  <td>    0.3927</td> <td>    0.011</td> <td>   34.537</td> <td> 0.000</td> <td>    0.370</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>  <td>   -1.3291</td> <td>    0.016</td> <td>  -84.888</td> <td> 0.000</td> <td>   -1.360</td> <td>   -1.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>  <td>   -0.4190</td> <td>    0.009</td> <td>  -46.101</td> <td> 0.000</td> <td>   -0.437</td> <td>   -0.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>  <td>   -2.0023</td> <td>    0.016</td> <td> -123.091</td> <td> 0.000</td> <td>   -2.034</td> <td>   -1.970</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>  <td>   -0.4784</td> <td>    0.010</td> <td>  -47.490</td> <td> 0.000</td> <td>   -0.498</td> <td>   -0.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>  <td>   -0.9116</td> <td>    0.016</td> <td>  -55.445</td> <td> 0.000</td> <td>   -0.944</td> <td>   -0.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>  <td>   -1.8392</td> <td>    0.023</td> <td>  -79.421</td> <td> 0.000</td> <td>   -1.885</td> <td>   -1.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>  <td>   -0.2885</td> <td>    0.015</td> <td>  -19.038</td> <td> 0.000</td> <td>   -0.318</td> <td>   -0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>  <td>    0.3894</td> <td>    0.012</td> <td>   32.847</td> <td> 0.000</td> <td>    0.366</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>  <td>   -0.0813</td> <td>    0.013</td> <td>   -6.189</td> <td> 0.000</td> <td>   -0.107</td> <td>   -0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>  <td>    0.3452</td> <td>    0.011</td> <td>   30.767</td> <td> 0.000</td> <td>    0.323</td> <td>    0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>  <td>    0.8484</td> <td>    0.015</td> <td>   55.949</td> <td> 0.000</td> <td>    0.819</td> <td>    0.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>  <td>    0.0697</td> <td>    0.010</td> <td>    7.020</td> <td> 0.000</td> <td>    0.050</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>  <td>    0.2467</td> <td>    0.017</td> <td>   14.570</td> <td> 0.000</td> <td>    0.214</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>  <td>   -1.0518</td> <td>    0.019</td> <td>  -54.119</td> <td> 0.000</td> <td>   -1.090</td> <td>   -1.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>  <td>   -0.1094</td> <td>    0.021</td> <td>   -5.290</td> <td> 0.000</td> <td>   -0.150</td> <td>   -0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>  <td>    0.3997</td> <td>    0.029</td> <td>   13.841</td> <td> 0.000</td> <td>    0.343</td> <td>    0.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>  <td>    0.3913</td> <td>    0.029</td> <td>   13.679</td> <td> 0.000</td> <td>    0.335</td> <td>    0.447</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.47 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               568630\n",
       "Model:                          Logit   Df Residuals:                   568601\n",
       "Method:                           MLE   Df Model:                           28\n",
       "Date:                Wed, 26 May 2021   Pseudo R-squ.:                  0.8859\n",
       "Time:                        23:17:30   Log-Likelihood:                -44982.\n",
       "converged:                       True   LL-Null:                   -3.9414e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Time       -4.918e-05   2.17e-07   -226.539      0.000   -4.96e-05   -4.88e-05\n",
       "V1             0.3778      0.011     34.073      0.000       0.356       0.400\n",
       "V2            -0.1572      0.010    -16.366      0.000      -0.176      -0.138\n",
       "V3            -0.9427      0.010    -90.684      0.000      -0.963      -0.922\n",
       "V4             0.9221      0.008    116.435      0.000       0.907       0.938\n",
       "V5             0.2028      0.010     20.666      0.000       0.184       0.222\n",
       "V6            -0.3329      0.008    -42.561      0.000      -0.348      -0.318\n",
       "V7            -0.2693      0.012    -21.929      0.000      -0.293      -0.245\n",
       "V8            -0.5386      0.010    -52.693      0.000      -0.559      -0.519\n",
       "V9            -0.6680      0.013    -51.931      0.000      -0.693      -0.643\n",
       "V10           -1.0888      0.017    -62.842      0.000      -1.123      -1.055\n",
       "V11            0.3927      0.011     34.537      0.000       0.370       0.415\n",
       "V12           -1.3291      0.016    -84.888      0.000      -1.360      -1.298\n",
       "V13           -0.4190      0.009    -46.101      0.000      -0.437      -0.401\n",
       "V14           -2.0023      0.016   -123.091      0.000      -2.034      -1.970\n",
       "V15           -0.4784      0.010    -47.490      0.000      -0.498      -0.459\n",
       "V16           -0.9116      0.016    -55.445      0.000      -0.944      -0.879\n",
       "V17           -1.8392      0.023    -79.421      0.000      -1.885      -1.794\n",
       "V18           -0.2885      0.015    -19.038      0.000      -0.318      -0.259\n",
       "V19            0.3894      0.012     32.847      0.000       0.366       0.413\n",
       "V20           -0.0813      0.013     -6.189      0.000      -0.107      -0.056\n",
       "V21            0.3452      0.011     30.767      0.000       0.323       0.367\n",
       "V22            0.8484      0.015     55.949      0.000       0.819       0.878\n",
       "V23            0.0697      0.010      7.020      0.000       0.050       0.089\n",
       "V24            0.2467      0.017     14.570      0.000       0.214       0.280\n",
       "V25           -1.0518      0.019    -54.119      0.000      -1.090      -1.014\n",
       "V26           -0.1094      0.021     -5.290      0.000      -0.150      -0.069\n",
       "V27            0.3997      0.029     13.841      0.000       0.343       0.456\n",
       "V28            0.3913      0.029     13.679      0.000       0.335       0.447\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.47 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_result.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CI 95%(2.5%)  CI 95%(97.5%)  Odds Ratio  pvalue\n",
      "Time      0.999950       0.999951    0.999951     0.0\n",
      "V1        1.427738       1.491165    1.459107     0.0\n",
      "V2        0.838635       0.870805    0.854569     0.0\n",
      "V3        0.381723       0.397599    0.389581     0.0\n",
      "V4        2.475796       2.553858    2.514524     0.0\n",
      "V5        1.201443       1.248548    1.224769     0.0\n",
      "V6        0.705926       0.727906    0.716832     0.0\n",
      "V7        0.745758       0.782534    0.763925     0.0\n",
      "V8        0.572009       0.595392    0.583583     0.0\n",
      "V9        0.499992       0.525847    0.512756     0.0\n",
      "V10       0.325365       0.348231    0.336604     0.0\n",
      "V11       1.448300       1.514307    1.480936     0.0\n",
      "V12       0.256707       0.272957    0.264707     0.0\n",
      "V13       0.646115       0.669547    0.657727     0.0\n",
      "V14       0.130785       0.139396    0.135022     0.0\n",
      "V15       0.607630       0.632106    0.619747     0.0\n",
      "V16       0.389131       0.415036    0.401875     0.0\n",
      "V17       0.151885       0.166318    0.158938     0.0\n",
      "V18       0.727439       0.771964    0.749371     0.0\n",
      "V19       1.442256       1.510867    1.476163     0.0\n",
      "V20       0.898446       0.945943    0.921888     0.0\n",
      "V21       1.381532       1.443645    1.412247     0.0\n",
      "V22       2.267605       2.406488    2.336015     0.0\n",
      "V23       1.051514       1.093234    1.072171     0.0\n",
      "V24       1.238027       1.322987    1.279802     0.0\n",
      "V25       0.336266       0.362884    0.349321     0.0\n",
      "V26       0.860795       0.933465    0.896394     0.0\n",
      "V27       1.409271       1.578172    1.491332     0.0\n",
      "V28       1.398320       1.564274    1.478971     0.0\n"
     ]
    }
   ],
   "source": [
    "#Interpretiranje rezultata\n",
    "\n",
    "b_params = np.exp(b_result.params)\n",
    "b_conf = np.exp(b_result.conf_int())\n",
    "b_conf['OR'] = b_params\n",
    "b_pvalue=round(b_result.pvalues,3)\n",
    "b_conf['pvalue']=b_pvalue\n",
    "b_conf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\n",
    "print ((b_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_new_features=bal[['Time','V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V20','V21', 'V22', 'V23', 'V25', 'V26', 'V27','Class']]\n",
    "b_x=b_new_features.iloc[:,:-1]\n",
    "b_y=b_new_features.iloc[:,-1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "b_x_train,b_x_test,b_y_train,b_y_test=train_test_split(b_x,b_y,test_size=.3,stratify=b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "b_logreg=LogisticRegression()\n",
    "b_logreg.fit(b_x_train,b_y_train)\n",
    "b_y_pred=b_logreg.predict(b_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.97     85295\n",
      "         1.0       0.98      0.96      0.97     85294\n",
      "\n",
      "    accuracy                           0.97    170589\n",
      "   macro avg       0.97      0.97      0.97    170589\n",
      "weighted avg       0.97      0.97      0.97    170589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluacija modela\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(b_y_test,b_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Matrica konfuzije\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "b_cm=confusion_matrix(b_y_test,b_y_pred)\n",
    "b_conf_matrix=pd.DataFrame(data=b_cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(b_conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_TN=b_cm[0,0]\n",
    "b_TP=b_cm[1,1]\n",
    "b_FN=b_cm[1,0]\n",
    "b_FP=b_cm[0,1]\n",
    "b_sensitivity=b_TP/float(b_TP+b_FN)\n",
    "b_specificity=b_TN/float(b_TN+b_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =        0.9730287415952963 \n",
      " The Missclassification = 1-Accuracy =                   0.026971258404703713 \n",
      " Sensitivity or True Positive Rate = TP/(TP+FN) =        0.9644289164536779 \n",
      " Specificity or True Negative Rate = TN/(TN+FP) =        0.9816284659124216 \n",
      " Positive Predictive value = TP/(TP+FP) =                0.9813067388788815 \n",
      " Negative predictive Value = TN/(TN+FN) =                0.9650307738410825 \n",
      " Positive Likelihood Ratio = Sensitivity/(1-Specificity) =  52.49582924627723 \n",
      " Negative likelihood Ratio = (1-Sensitivity)/Specificity =  0.036236809324043856\n"
     ]
    }
   ],
   "source": [
    "print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =       ',(b_TP+b_TN)/float(b_TP+b_TN+b_FP+b_FN),'\\n',\n",
    "\n",
    "'The Missclassification = 1-Accuracy =                  ',1-((b_TP+b_TN)/float(b_TP+b_TN+b_FP+b_FN)),'\\n',\n",
    "\n",
    "'Sensitivity or True Positive Rate = TP/(TP+FN) =       ',b_TP/float(b_TP+b_FN),'\\n',\n",
    "\n",
    "'Specificity or True Negative Rate = TN/(TN+FP) =       ',b_TN/float(b_TN+b_FP),'\\n',\n",
    "\n",
    "'Positive Predictive value = TP/(TP+FP) =               ',b_TP/float(b_TP+b_FP),'\\n',\n",
    "\n",
    "'Negative predictive Value = TN/(TN+FN) =               ',b_TN/float(b_TN+b_FN),'\\n',\n",
    "\n",
    "'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',b_sensitivity/(1-b_specificity),'\\n',\n",
    "      \n",
    "'Negative likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-b_sensitivity)/b_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob of Not Fraud (0)</th>\n",
       "      <th>Prob of Fraud (1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998826</td>\n",
       "      <td>0.001174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.969498</td>\n",
       "      <td>0.030502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999252</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999742</td>\n",
       "      <td>0.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.995287</td>\n",
       "      <td>0.004713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prob of Not Fraud (0)  Prob of Fraud (1)\n",
       "0               0.998826           0.001174\n",
       "1               0.969498           0.030502\n",
       "2               0.999252           0.000748\n",
       "3               0.999742           0.000258\n",
       "4               0.995287           0.004713"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_y_pred_prob=b_logreg.predict_proba(b_x_test)[:,:]\n",
    "b_y_pred_prob_df=pd.DataFrame(data=b_y_pred_prob, columns=['Prob of Not Fraud (0)','Prob of Fraud (1)'])\n",
    "b_y_pred_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 0.0 threshold the Confusion Matrix is  \n",
      " [[    0 85295]\n",
      " [    0 85294]] \n",
      " with 85294 correct predictions and 0 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  1.0 Specificity:  0.0 \n",
      "\n",
      "\n",
      "\n",
      "With 0.1 threshold the Confusion Matrix is  \n",
      " [[74838 10457]\n",
      " [  888 84406]] \n",
      " with 159244 correct predictions and 888 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.989588951157174 Specificity:  0.8774019579107802 \n",
      "\n",
      "\n",
      "\n",
      "With 0.2 threshold the Confusion Matrix is  \n",
      " [[80285  5010]\n",
      " [ 1495 83799]] \n",
      " with 164084 correct predictions and 1495 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9824723896170892 Specificity:  0.9412626765930008 \n",
      "\n",
      "\n",
      "\n",
      "With 0.3 threshold the Confusion Matrix is  \n",
      " [[82143  3152]\n",
      " [ 2005 83289]] \n",
      " with 165432 correct predictions and 2005 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9764930710249256 Specificity:  0.9630458995251773 \n",
      "\n",
      "\n",
      "\n",
      "With 0.4 threshold the Confusion Matrix is  \n",
      " [[83098  2197]\n",
      " [ 2590 82704]] \n",
      " with 165802 correct predictions and 2590 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9696344408750909 Specificity:  0.9742423354241163 \n",
      "\n",
      "\n",
      "\n",
      "With 0.5 threshold the Confusion Matrix is  \n",
      " [[83728  1567]\n",
      " [ 3034 82260]] \n",
      " with 165988 correct predictions and 3034 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9644289164536779 Specificity:  0.9816284659124216 \n",
      "\n",
      "\n",
      "\n",
      "With 0.6 threshold the Confusion Matrix is  \n",
      " [[84106  1189]\n",
      " [ 3391 81903]] \n",
      " with 166009 correct predictions and 3391 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9602433934391633 Specificity:  0.9860601442054048 \n",
      "\n",
      "\n",
      "\n",
      "With 0.7 threshold the Confusion Matrix is  \n",
      " [[84374   921]\n",
      " [ 3787 81507]] \n",
      " with 165881 correct predictions and 3787 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9556006284146599 Specificity:  0.9892021806670965 \n",
      "\n",
      "\n",
      "\n",
      "With 0.8 threshold the Confusion Matrix is  \n",
      " [[84549   746]\n",
      " [ 4386 80908]] \n",
      " with 165457 correct predictions and 4386 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9485778601073932 Specificity:  0.9912538835805147 \n",
      "\n",
      "\n",
      "\n",
      "With 0.9 threshold the Confusion Matrix is  \n",
      " [[84756   539]\n",
      " [ 5218 80076]] \n",
      " with 164832 correct predictions and 5218 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.938823363894295 Specificity:  0.9936807550266722 \n",
      "\n",
      "\n",
      "\n",
      "With 1.0 threshold the Confusion Matrix is  \n",
      " [[85295     0]\n",
      " [85294     0]] \n",
      " with 85295 correct predictions and 85294 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.0 Specificity:  1.0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "for i in range(0,11):\n",
    "    b_cm2=0\n",
    "    b_y_pred_prob_yes=b_logreg.predict_proba(b_x_test)\n",
    "    b_y_pred2=binarize(b_y_pred_prob_yes,i/10)[:,1]\n",
    "    b_cm2=confusion_matrix(b_y_test,b_y_pred2)\n",
    "    print ('With',i/10,'threshold the Confusion Matrix is ','\\n',b_cm2,'\\n',\n",
    "            'with',b_cm2[0,0]+b_cm2[1,1],'correct predictions and',b_cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n",
    "          'Sensitivity: ',b_cm2[1,1]/(float(b_cm2[1,1]+b_cm2[1,0])),'Specificity: ',b_cm2[0,0]/(float(b_cm2[0,0]+b_cm2[0,1])),'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "b_fpr, b_tpr, thresholds = roc_curve(b_y_test, b_y_pred_prob_yes[:,1])\n",
    "plt.plot(b_fpr,b_tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for Fraud classifier - Balanced')\n",
    "plt.xlabel('False positive rate (1-Specificity)')\n",
    "plt.ylabel('True positive rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994913995285154"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(b_y_test,b_y_pred_prob_yes[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62    0.0  \n",
       "1    2.69    0.0  \n",
       "2  378.66    0.0  \n",
       "3  123.50    0.0  \n",
       "4   69.99    0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BALANSIRANI PODACI SA NORMALIZOVANOM AMOUNT KOLONOM\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "normbal = pd.read_csv(r'C:\\Users\\pc\\Downloads\\smote_amount_creditcard.csv')\n",
    "normbal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_estimators=['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
    "\n",
    "nb_X1 = normbal[nb_estimators]\n",
    "nb_y = normbal['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_col=nb_X1.columns[:-1]\n",
    "nb_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.061623\n",
      "         Iterations 15\n"
     ]
    }
   ],
   "source": [
    "nb_X = sm.add_constant(nb_X1)\n",
    "nb_reg_logit = sm.Logit(nb_y,nb_X)\n",
    "nb_results_logit = nb_reg_logit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>   <td>568630</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>568599</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    30</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 26 May 2021</td> <th>  Pseudo R-squ.:     </th>   <td>0.9111</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>23:39:38</td>     <th>  Log-Likelihood:    </th>  <td> -35041.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-3.9414e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>  <td>   -5.7778</td> <td>    0.054</td> <td> -107.365</td> <td> 0.000</td> <td>   -5.883</td> <td>   -5.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th>   <td>-1.015e-05</td> <td> 3.39e-07</td> <td>  -29.909</td> <td> 0.000</td> <td>-1.08e-05</td> <td>-9.49e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>     <td>    1.4536</td> <td>    0.023</td> <td>   63.741</td> <td> 0.000</td> <td>    1.409</td> <td>    1.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>     <td>    1.2828</td> <td>    0.040</td> <td>   32.443</td> <td> 0.000</td> <td>    1.205</td> <td>    1.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>     <td>    0.7809</td> <td>    0.019</td> <td>   42.169</td> <td> 0.000</td> <td>    0.745</td> <td>    0.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>     <td>    0.7863</td> <td>    0.013</td> <td>   61.174</td> <td> 0.000</td> <td>    0.761</td> <td>    0.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>     <td>    1.4624</td> <td>    0.028</td> <td>   51.835</td> <td> 0.000</td> <td>    1.407</td> <td>    1.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>     <td>   -0.9020</td> <td>    0.019</td> <td>  -47.157</td> <td> 0.000</td> <td>   -0.940</td> <td>   -0.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>     <td>   -1.4123</td> <td>    0.036</td> <td>  -39.718</td> <td> 0.000</td> <td>   -1.482</td> <td>   -1.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>     <td>   -0.4989</td> <td>    0.012</td> <td>  -40.309</td> <td> 0.000</td> <td>   -0.523</td> <td>   -0.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>     <td>   -0.5752</td> <td>    0.018</td> <td>  -32.223</td> <td> 0.000</td> <td>   -0.610</td> <td>   -0.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>    <td>   -1.1835</td> <td>    0.023</td> <td>  -50.643</td> <td> 0.000</td> <td>   -1.229</td> <td>   -1.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>    <td>    0.8999</td> <td>    0.015</td> <td>   61.583</td> <td> 0.000</td> <td>    0.871</td> <td>    0.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>    <td>   -1.8083</td> <td>    0.020</td> <td>  -90.655</td> <td> 0.000</td> <td>   -1.847</td> <td>   -1.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>    <td>   -0.5546</td> <td>    0.011</td> <td>  -50.733</td> <td> 0.000</td> <td>   -0.576</td> <td>   -0.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>    <td>   -2.2760</td> <td>    0.022</td> <td> -101.593</td> <td> 0.000</td> <td>   -2.320</td> <td>   -2.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>    <td>   -0.2230</td> <td>    0.012</td> <td>  -18.253</td> <td> 0.000</td> <td>   -0.247</td> <td>   -0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>    <td>   -1.1337</td> <td>    0.023</td> <td>  -50.194</td> <td> 0.000</td> <td>   -1.178</td> <td>   -1.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>    <td>   -1.9664</td> <td>    0.027</td> <td>  -72.820</td> <td> 0.000</td> <td>   -2.019</td> <td>   -1.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>    <td>   -0.7489</td> <td>    0.020</td> <td>  -37.065</td> <td> 0.000</td> <td>   -0.789</td> <td>   -0.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>    <td>    0.7323</td> <td>    0.017</td> <td>   43.866</td> <td> 0.000</td> <td>    0.700</td> <td>    0.765</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>    <td>   -1.4488</td> <td>    0.036</td> <td>  -39.699</td> <td> 0.000</td> <td>   -1.520</td> <td>   -1.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>    <td>   -0.0147</td> <td>    0.015</td> <td>   -1.003</td> <td> 0.316</td> <td>   -0.043</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>    <td>    1.0613</td> <td>    0.021</td> <td>   50.363</td> <td> 0.000</td> <td>    1.020</td> <td>    1.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>    <td>    1.3350</td> <td>    0.035</td> <td>   38.147</td> <td> 0.000</td> <td>    1.266</td> <td>    1.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>    <td>    0.0828</td> <td>    0.022</td> <td>    3.742</td> <td> 0.000</td> <td>    0.039</td> <td>    0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>    <td>    0.1443</td> <td>    0.024</td> <td>    5.923</td> <td> 0.000</td> <td>    0.097</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>    <td>   -0.2182</td> <td>    0.026</td> <td>   -8.296</td> <td> 0.000</td> <td>   -0.270</td> <td>   -0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>    <td>   -0.0524</td> <td>    0.042</td> <td>   -1.250</td> <td> 0.211</td> <td>   -0.135</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>    <td>    1.6534</td> <td>    0.048</td> <td>   34.701</td> <td> 0.000</td> <td>    1.560</td> <td>    1.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amount</th> <td>    0.0165</td> <td>    0.000</td> <td>   43.774</td> <td> 0.000</td> <td>    0.016</td> <td>    0.017</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.48 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               568630\n",
       "Model:                          Logit   Df Residuals:                   568599\n",
       "Method:                           MLE   Df Model:                           30\n",
       "Date:                Wed, 26 May 2021   Pseudo R-squ.:                  0.9111\n",
       "Time:                        23:39:38   Log-Likelihood:                -35041.\n",
       "converged:                       True   LL-Null:                   -3.9414e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -5.7778      0.054   -107.365      0.000      -5.883      -5.672\n",
       "Time       -1.015e-05   3.39e-07    -29.909      0.000   -1.08e-05   -9.49e-06\n",
       "V1             1.4536      0.023     63.741      0.000       1.409       1.498\n",
       "V2             1.2828      0.040     32.443      0.000       1.205       1.360\n",
       "V3             0.7809      0.019     42.169      0.000       0.745       0.817\n",
       "V4             0.7863      0.013     61.174      0.000       0.761       0.811\n",
       "V5             1.4624      0.028     51.835      0.000       1.407       1.518\n",
       "V6            -0.9020      0.019    -47.157      0.000      -0.940      -0.865\n",
       "V7            -1.4123      0.036    -39.718      0.000      -1.482      -1.343\n",
       "V8            -0.4989      0.012    -40.309      0.000      -0.523      -0.475\n",
       "V9            -0.5752      0.018    -32.223      0.000      -0.610      -0.540\n",
       "V10           -1.1835      0.023    -50.643      0.000      -1.229      -1.138\n",
       "V11            0.8999      0.015     61.583      0.000       0.871       0.929\n",
       "V12           -1.8083      0.020    -90.655      0.000      -1.847      -1.769\n",
       "V13           -0.5546      0.011    -50.733      0.000      -0.576      -0.533\n",
       "V14           -2.2760      0.022   -101.593      0.000      -2.320      -2.232\n",
       "V15           -0.2230      0.012    -18.253      0.000      -0.247      -0.199\n",
       "V16           -1.1337      0.023    -50.194      0.000      -1.178      -1.089\n",
       "V17           -1.9664      0.027    -72.820      0.000      -2.019      -1.914\n",
       "V18           -0.7489      0.020    -37.065      0.000      -0.789      -0.709\n",
       "V19            0.7323      0.017     43.866      0.000       0.700       0.765\n",
       "V20           -1.4488      0.036    -39.699      0.000      -1.520      -1.377\n",
       "V21           -0.0147      0.015     -1.003      0.316      -0.043       0.014\n",
       "V22            1.0613      0.021     50.363      0.000       1.020       1.103\n",
       "V23            1.3350      0.035     38.147      0.000       1.266       1.404\n",
       "V24            0.0828      0.022      3.742      0.000       0.039       0.126\n",
       "V25            0.1443      0.024      5.923      0.000       0.097       0.192\n",
       "V26           -0.2182      0.026     -8.296      0.000      -0.270      -0.167\n",
       "V27           -0.0524      0.042     -1.250      0.211      -0.135       0.030\n",
       "V28            1.6534      0.048     34.701      0.000       1.560       1.747\n",
       "Amount         0.0165      0.000     43.774      0.000       0.016       0.017\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.48 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_results_logit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_back_feature_elem (data_frame,dep_var,col_list):\n",
    "    \n",
    "\n",
    "    while len(col_list)>0 :\n",
    "        nb_model=sm.Logit(dep_var,data_frame[col_list])\n",
    "        nb_result=nb_model.fit(disp=0)\n",
    "        nb_largest_pvalue=round(nb_result.pvalues,3).nlargest(1)\n",
    "        if nb_largest_pvalue[0]<(0.0001):\n",
    "            return nb_result\n",
    "            break\n",
    "        else:\n",
    "            col_list=col_list.drop(nb_largest_pvalue.index)\n",
    "\n",
    "nb_result=nb_back_feature_elem(nb_X,bal.Class,nb_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Class</td>      <th>  No. Observations:  </th>   <td>568630</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>568601</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    28</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 26 May 2021</td> <th>  Pseudo R-squ.:     </th>   <td>0.8859</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>23:43:50</td>     <th>  Log-Likelihood:    </th>  <td> -44982.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-3.9414e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time</th> <td>-4.918e-05</td> <td> 2.17e-07</td> <td> -226.539</td> <td> 0.000</td> <td>-4.96e-05</td> <td>-4.88e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V1</th>   <td>    0.3778</td> <td>    0.011</td> <td>   34.073</td> <td> 0.000</td> <td>    0.356</td> <td>    0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V2</th>   <td>   -0.1572</td> <td>    0.010</td> <td>  -16.366</td> <td> 0.000</td> <td>   -0.176</td> <td>   -0.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V3</th>   <td>   -0.9427</td> <td>    0.010</td> <td>  -90.684</td> <td> 0.000</td> <td>   -0.963</td> <td>   -0.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V4</th>   <td>    0.9221</td> <td>    0.008</td> <td>  116.435</td> <td> 0.000</td> <td>    0.907</td> <td>    0.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V5</th>   <td>    0.2028</td> <td>    0.010</td> <td>   20.666</td> <td> 0.000</td> <td>    0.184</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V6</th>   <td>   -0.3329</td> <td>    0.008</td> <td>  -42.561</td> <td> 0.000</td> <td>   -0.348</td> <td>   -0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V7</th>   <td>   -0.2693</td> <td>    0.012</td> <td>  -21.929</td> <td> 0.000</td> <td>   -0.293</td> <td>   -0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V8</th>   <td>   -0.5386</td> <td>    0.010</td> <td>  -52.693</td> <td> 0.000</td> <td>   -0.559</td> <td>   -0.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V9</th>   <td>   -0.6680</td> <td>    0.013</td> <td>  -51.931</td> <td> 0.000</td> <td>   -0.693</td> <td>   -0.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V10</th>  <td>   -1.0888</td> <td>    0.017</td> <td>  -62.842</td> <td> 0.000</td> <td>   -1.123</td> <td>   -1.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V11</th>  <td>    0.3927</td> <td>    0.011</td> <td>   34.537</td> <td> 0.000</td> <td>    0.370</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V12</th>  <td>   -1.3291</td> <td>    0.016</td> <td>  -84.888</td> <td> 0.000</td> <td>   -1.360</td> <td>   -1.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V13</th>  <td>   -0.4190</td> <td>    0.009</td> <td>  -46.101</td> <td> 0.000</td> <td>   -0.437</td> <td>   -0.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V14</th>  <td>   -2.0023</td> <td>    0.016</td> <td> -123.091</td> <td> 0.000</td> <td>   -2.034</td> <td>   -1.970</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V15</th>  <td>   -0.4784</td> <td>    0.010</td> <td>  -47.490</td> <td> 0.000</td> <td>   -0.498</td> <td>   -0.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V16</th>  <td>   -0.9116</td> <td>    0.016</td> <td>  -55.445</td> <td> 0.000</td> <td>   -0.944</td> <td>   -0.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V17</th>  <td>   -1.8392</td> <td>    0.023</td> <td>  -79.421</td> <td> 0.000</td> <td>   -1.885</td> <td>   -1.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V18</th>  <td>   -0.2885</td> <td>    0.015</td> <td>  -19.038</td> <td> 0.000</td> <td>   -0.318</td> <td>   -0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V19</th>  <td>    0.3894</td> <td>    0.012</td> <td>   32.847</td> <td> 0.000</td> <td>    0.366</td> <td>    0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V20</th>  <td>   -0.0813</td> <td>    0.013</td> <td>   -6.189</td> <td> 0.000</td> <td>   -0.107</td> <td>   -0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V21</th>  <td>    0.3452</td> <td>    0.011</td> <td>   30.767</td> <td> 0.000</td> <td>    0.323</td> <td>    0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V22</th>  <td>    0.8484</td> <td>    0.015</td> <td>   55.949</td> <td> 0.000</td> <td>    0.819</td> <td>    0.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V23</th>  <td>    0.0697</td> <td>    0.010</td> <td>    7.020</td> <td> 0.000</td> <td>    0.050</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V24</th>  <td>    0.2467</td> <td>    0.017</td> <td>   14.570</td> <td> 0.000</td> <td>    0.214</td> <td>    0.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V25</th>  <td>   -1.0518</td> <td>    0.019</td> <td>  -54.119</td> <td> 0.000</td> <td>   -1.090</td> <td>   -1.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V26</th>  <td>   -0.1094</td> <td>    0.021</td> <td>   -5.290</td> <td> 0.000</td> <td>   -0.150</td> <td>   -0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V27</th>  <td>    0.3997</td> <td>    0.029</td> <td>   13.841</td> <td> 0.000</td> <td>    0.343</td> <td>    0.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>V28</th>  <td>    0.3913</td> <td>    0.029</td> <td>   13.679</td> <td> 0.000</td> <td>    0.335</td> <td>    0.447</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.47 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  Class   No. Observations:               568630\n",
       "Model:                          Logit   Df Residuals:                   568601\n",
       "Method:                           MLE   Df Model:                           28\n",
       "Date:                Wed, 26 May 2021   Pseudo R-squ.:                  0.8859\n",
       "Time:                        23:43:50   Log-Likelihood:                -44982.\n",
       "converged:                       True   LL-Null:                   -3.9414e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Time       -4.918e-05   2.17e-07   -226.539      0.000   -4.96e-05   -4.88e-05\n",
       "V1             0.3778      0.011     34.073      0.000       0.356       0.400\n",
       "V2            -0.1572      0.010    -16.366      0.000      -0.176      -0.138\n",
       "V3            -0.9427      0.010    -90.684      0.000      -0.963      -0.922\n",
       "V4             0.9221      0.008    116.435      0.000       0.907       0.938\n",
       "V5             0.2028      0.010     20.666      0.000       0.184       0.222\n",
       "V6            -0.3329      0.008    -42.561      0.000      -0.348      -0.318\n",
       "V7            -0.2693      0.012    -21.929      0.000      -0.293      -0.245\n",
       "V8            -0.5386      0.010    -52.693      0.000      -0.559      -0.519\n",
       "V9            -0.6680      0.013    -51.931      0.000      -0.693      -0.643\n",
       "V10           -1.0888      0.017    -62.842      0.000      -1.123      -1.055\n",
       "V11            0.3927      0.011     34.537      0.000       0.370       0.415\n",
       "V12           -1.3291      0.016    -84.888      0.000      -1.360      -1.298\n",
       "V13           -0.4190      0.009    -46.101      0.000      -0.437      -0.401\n",
       "V14           -2.0023      0.016   -123.091      0.000      -2.034      -1.970\n",
       "V15           -0.4784      0.010    -47.490      0.000      -0.498      -0.459\n",
       "V16           -0.9116      0.016    -55.445      0.000      -0.944      -0.879\n",
       "V17           -1.8392      0.023    -79.421      0.000      -1.885      -1.794\n",
       "V18           -0.2885      0.015    -19.038      0.000      -0.318      -0.259\n",
       "V19            0.3894      0.012     32.847      0.000       0.366       0.413\n",
       "V20           -0.0813      0.013     -6.189      0.000      -0.107      -0.056\n",
       "V21            0.3452      0.011     30.767      0.000       0.323       0.367\n",
       "V22            0.8484      0.015     55.949      0.000       0.819       0.878\n",
       "V23            0.0697      0.010      7.020      0.000       0.050       0.089\n",
       "V24            0.2467      0.017     14.570      0.000       0.214       0.280\n",
       "V25           -1.0518      0.019    -54.119      0.000      -1.090      -1.014\n",
       "V26           -0.1094      0.021     -5.290      0.000      -0.150      -0.069\n",
       "V27            0.3997      0.029     13.841      0.000       0.343       0.456\n",
       "V28            0.3913      0.029     13.679      0.000       0.335       0.447\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.47 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_result.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CI 95%(2.5%)  CI 95%(97.5%)  Odds Ratio  pvalue\n",
      "Time      0.999950       0.999951    0.999951     0.0\n",
      "V1        1.427738       1.491165    1.459107     0.0\n",
      "V2        0.838635       0.870805    0.854569     0.0\n",
      "V3        0.381723       0.397599    0.389581     0.0\n",
      "V4        2.475796       2.553858    2.514524     0.0\n",
      "V5        1.201443       1.248548    1.224769     0.0\n",
      "V6        0.705926       0.727906    0.716832     0.0\n",
      "V7        0.745758       0.782534    0.763925     0.0\n",
      "V8        0.572009       0.595392    0.583583     0.0\n",
      "V9        0.499992       0.525847    0.512756     0.0\n",
      "V10       0.325365       0.348231    0.336604     0.0\n",
      "V11       1.448300       1.514307    1.480936     0.0\n",
      "V12       0.256707       0.272957    0.264707     0.0\n",
      "V13       0.646115       0.669547    0.657727     0.0\n",
      "V14       0.130785       0.139396    0.135022     0.0\n",
      "V15       0.607630       0.632106    0.619747     0.0\n",
      "V16       0.389131       0.415036    0.401875     0.0\n",
      "V17       0.151885       0.166318    0.158938     0.0\n",
      "V18       0.727439       0.771964    0.749371     0.0\n",
      "V19       1.442256       1.510867    1.476163     0.0\n",
      "V20       0.898446       0.945943    0.921888     0.0\n",
      "V21       1.381532       1.443645    1.412247     0.0\n",
      "V22       2.267605       2.406488    2.336015     0.0\n",
      "V23       1.051514       1.093234    1.072171     0.0\n",
      "V24       1.238027       1.322987    1.279802     0.0\n",
      "V25       0.336266       0.362884    0.349321     0.0\n",
      "V26       0.860795       0.933465    0.896394     0.0\n",
      "V27       1.409271       1.578172    1.491332     0.0\n",
      "V28       1.398320       1.564274    1.478971     0.0\n"
     ]
    }
   ],
   "source": [
    "#Interpretiranje rezultata\n",
    "\n",
    "nb_params = np.exp(nb_result.params)\n",
    "nb_conf = np.exp(nb_result.conf_int())\n",
    "nb_conf['OR'] = nb_params\n",
    "nb_pvalue=round(nb_result.pvalues,3)\n",
    "nb_conf['pvalue']=nb_pvalue\n",
    "nb_conf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\n",
    "print ((nb_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_new_features=normbal[['Time','V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V20','V21', 'V22', 'V23', 'V25', 'V26', 'V27','Class']]\n",
    "nb_x=nb_new_features.iloc[:,:-1]\n",
    "nb_y=nb_new_features.iloc[:,-1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "nb_x_train,nb_x_test,nb_y_train,nb_y_test=train_test_split(nb_x,nb_y,test_size=.3,stratify=nb_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "nb_logreg=LogisticRegression()\n",
    "nb_logreg.fit(nb_x_train,nb_y_train)\n",
    "nb_y_pred=nb_logreg.predict(nb_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.97      0.96     85295\n",
      "         1.0       0.97      0.95      0.96     85294\n",
      "\n",
      "    accuracy                           0.96    170589\n",
      "   macro avg       0.96      0.96      0.96    170589\n",
      "weighted avg       0.96      0.96      0.96    170589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluacija modela\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(nb_y_test,nb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Matrica konfuzije\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "nb_cm=confusion_matrix(nb_y_test,nb_y_pred)\n",
    "nb_conf_matrix=pd.DataFrame(data=nb_cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.heatmap(nb_conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_TN=nb_cm[0,0]\n",
    "nb_TP=nb_cm[1,1]\n",
    "nb_FN=nb_cm[1,0]\n",
    "nb_FP=nb_cm[0,1]\n",
    "nb_sensitivity=nb_TP/float(nb_TP+nb_FN)\n",
    "nb_specificity=nb_TN/float(nb_TN+nb_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =        0.9585377720720563 \n",
      " The Missclassification = 1-Accuracy =                   0.04146222792794374 \n",
      " Sensitivity or True Positive Rate = TP/(TP+FN) =        0.9493282059699393 \n",
      " Specificity or True Negative Rate = TN/(TN+FP) =        0.9677472302010669 \n",
      " Positive Predictive value = TP/(TP+FP) =                0.967141645664871 \n",
      " Negative predictive Value = TN/(TN+FN) =                0.9502452052586743 \n",
      " Positive Likelihood Ratio = Sensitivity/(1-Specificity) =  29.434005571866958 \n",
      " Negative likelihood Ratio = (1-Sensitivity)/Specificity =  0.05236056735551984\n"
     ]
    }
   ],
   "source": [
    "print('The acuuracy of the model = TP+TN/(TP+TN+FP+FN) =       ',(nb_TP+nb_TN)/float(nb_TP+nb_TN+nb_FP+nb_FN),'\\n',\n",
    "\n",
    "'The Missclassification = 1-Accuracy =                  ',1-((nb_TP+nb_TN)/float(nb_TP+nb_TN+nb_FP+nb_FN)),'\\n',\n",
    "\n",
    "'Sensitivity or True Positive Rate = TP/(TP+FN) =       ',nb_TP/float(nb_TP+nb_FN),'\\n',\n",
    "\n",
    "'Specificity or True Negative Rate = TN/(TN+FP) =       ',nb_TN/float(nb_TN+nb_FP),'\\n',\n",
    "\n",
    "'Positive Predictive value = TP/(TP+FP) =               ',nb_TP/float(nb_TP+nb_FP),'\\n',\n",
    "\n",
    "'Negative predictive Value = TN/(TN+FN) =               ',nb_TN/float(nb_TN+nb_FN),'\\n',\n",
    "\n",
    "'Positive Likelihood Ratio = Sensitivity/(1-Specificity) = ',nb_sensitivity/(1-nb_specificity),'\\n',\n",
    "      \n",
    "'Negative likelihood Ratio = (1-Sensitivity)/Specificity = ',(1-nb_sensitivity)/nb_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prob of Not Fraud (0)</th>\n",
       "      <th>Prob of Fraud (1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.500257e-01</td>\n",
       "      <td>0.249974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.907681e-01</td>\n",
       "      <td>0.009232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.332268e-15</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.828802e-01</td>\n",
       "      <td>0.117120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prob of Not Fraud (0)  Prob of Fraud (1)\n",
       "0           7.500257e-01           0.249974\n",
       "1           9.907681e-01           0.009232\n",
       "2           0.000000e+00           1.000000\n",
       "3           1.332268e-15           1.000000\n",
       "4           8.828802e-01           0.117120"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_y_pred_prob=nb_logreg.predict_proba(nb_x_test)[:,:]\n",
    "nb_y_pred_prob_df=pd.DataFrame(data=nb_y_pred_prob, columns=['Prob of Not Fraud (0)','Prob of Fraud (1)'])\n",
    "nb_y_pred_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 0.0 threshold the Confusion Matrix is  \n",
      " [[    0 85295]\n",
      " [    0 85294]] \n",
      " with 85294 correct predictions and 0 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  1.0 Specificity:  0.0 \n",
      "\n",
      "\n",
      "\n",
      "With 0.1 threshold the Confusion Matrix is  \n",
      " [[59934 25361]\n",
      " [ 1285 84009]] \n",
      " with 143943 correct predictions and 1285 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9849344619785683 Specificity:  0.7026672137874436 \n",
      "\n",
      "\n",
      "\n",
      "With 0.2 threshold the Confusion Matrix is  \n",
      " [[73514 11781]\n",
      " [ 2233 83061]] \n",
      " with 156575 correct predictions and 2233 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9738199638896053 Specificity:  0.861879359868691 \n",
      "\n",
      "\n",
      "\n",
      "With 0.3 threshold the Confusion Matrix is  \n",
      " [[78385  6910]\n",
      " [ 2934 82360]] \n",
      " with 160745 correct predictions and 2934 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9656013318639061 Specificity:  0.9189870449616039 \n",
      "\n",
      "\n",
      "\n",
      "With 0.4 threshold the Confusion Matrix is  \n",
      " [[81063  4232]\n",
      " [ 3849 81445]] \n",
      " with 162508 correct predictions and 3849 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9548737308603185 Specificity:  0.9503839615452254 \n",
      "\n",
      "\n",
      "\n",
      "With 0.5 threshold the Confusion Matrix is  \n",
      " [[82544  2751]\n",
      " [ 4322 80972]] \n",
      " with 163516 correct predictions and 4322 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9493282059699393 Specificity:  0.9677472302010669 \n",
      "\n",
      "\n",
      "\n",
      "With 0.6 threshold the Confusion Matrix is  \n",
      " [[83297  1998]\n",
      " [ 4891 80403]] \n",
      " with 163700 correct predictions and 4891 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.942657162285741 Specificity:  0.976575414737089 \n",
      "\n",
      "\n",
      "\n",
      "With 0.7 threshold the Confusion Matrix is  \n",
      " [[83836  1459]\n",
      " [ 5424 79870]] \n",
      " with 163706 correct predictions and 5424 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.936408188149225 Specificity:  0.9828946597104168 \n",
      "\n",
      "\n",
      "\n",
      "With 0.8 threshold the Confusion Matrix is  \n",
      " [[84240  1055]\n",
      " [ 6721 78573]] \n",
      " with 162813 correct predictions and 6721 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9212019602785659 Specificity:  0.9876311624362507 \n",
      "\n",
      "\n",
      "\n",
      "With 0.9 threshold the Confusion Matrix is  \n",
      " [[84614   681]\n",
      " [ 7674 77620]] \n",
      " with 162234 correct predictions and 7674 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.9100288414190916 Specificity:  0.9920159446626414 \n",
      "\n",
      "\n",
      "\n",
      "With 1.0 threshold the Confusion Matrix is  \n",
      " [[85295     0]\n",
      " [85294     0]] \n",
      " with 85295 correct predictions and 85294 Type II errors( False Negatives) \n",
      "\n",
      " Sensitivity:  0.0 Specificity:  1.0 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "for i in range(0,11):\n",
    "    nb_cm2=0\n",
    "    nb_y_pred_prob_yes=nb_logreg.predict_proba(nb_x_test)\n",
    "    nb_y_pred2=binarize(nb_y_pred_prob_yes,i/10)[:,1]\n",
    "    nb_cm2=confusion_matrix(nb_y_test,nb_y_pred2)\n",
    "    print ('With',i/10,'threshold the Confusion Matrix is ','\\n',nb_cm2,'\\n',\n",
    "            'with',nb_cm2[0,0]+nb_cm2[1,1],'correct predictions and',nb_cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n",
    "          'Sensitivity: ',nb_cm2[1,1]/(float(nb_cm2[1,1]+nb_cm2[1,0])),'Specificity: ',nb_cm2[0,0]/(float(nb_cm2[0,0]+nb_cm2[0,1])),'\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "nb_fpr, nb_tpr, thresholds = roc_curve(nb_y_test, nb_y_pred_prob_yes[:,1])\n",
    "plt.plot(nb_fpr,nb_tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for Fraud classifier - Balanced and Normalized')\n",
    "plt.xlabel('False positive rate (1-Specificity)')\n",
    "plt.ylabel('True positive rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866910168896229"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(nb_y_test,nb_y_pred_prob_yes[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
